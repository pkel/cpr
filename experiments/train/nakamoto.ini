# Comments starting with ; are valid configuration stanzas after removing the
# leading ;. Comments starting with # are explanations and should not be
# commented out.

[wandb]
; tags = list, your, tags, here

[main]
env = cpr_gym:core-v0
protocol = nakamoto
; protocol = ethereum
; protocol = bk
; protocol = bkll
; protocol = tailstorm
; protocol = tailstormll
; alpha = .33
alpha_min = .1
alpha_max = .5
; alpha_schedule = .1, .2, .33, .44
; reward = sparse_relative
reward = sparse_per_progress
; reward = dense_per_progress
episode_len = 128
total_timesteps = 1e7
# n_envs = psutil.cpu_count()

[env_args]
activation_delay = 1
defenders = 2
gamma = 0.5

[eval]
# Run evaluation once per freq iterations
freq = 12

# We evaluate on deterministic policies. Early during training, the policy
# might be very dumb. It seems to cause massive simulation overhead. Thus we
# skip evaluation in the beginning.
start_at_iteration = 1

# If we train on single alpha or list of alphas we evaluate on all alphas.
# If we train on a range of alphas we use alpha_step to derive a list of
# alphas to evaluate on.
alpha_step = 0.025

# How many episodes per evaluation per n_env per alpha?
episodes_per_alpha_per_env = 5

# For reporting, we maintain a ring buffer of past episodes in each n_env.
# When reporting per_alpha statistics, we calculate the mean over this ring
# buffer. The number of recorded episodes is
# episodes_per_alpha_per_env * # alphas * n_envs * recorder_multiple
recorder_multiple = 1

# Reporting per_alpha statistics for all alphas might overload the dashboard.
# Set report_alpha = n to report only every n-th alpha.
report_alpha = 2

[protocol_args]
# Set number of puzzles per block for bk[ll], and tailstorm[ll].
; k = 8

# Set reward scheme for ethereum, bk[ll], and tailstorm[ll].
; reward = constant
; reward = discount

# Choose sub-block selection algorithm for tailstorm[ll].
; subblock_selection = altruistic
; subblock_selection = heuristic
; subblock_selection = optimal

[ppo]
batch_size = 4096
n_steps_multiple = 1
layer_size = 16
n_layers = 3
gamma = 0.999
starting_lr = 1e-3
ending_lr = 1e-5
