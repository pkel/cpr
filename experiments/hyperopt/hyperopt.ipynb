{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "293f26ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local-v0.5.4-4-g68df215'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cpr_gym\n",
    "\n",
    "cpr_gym.engine.cpr_lib_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bf122ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nakamoto consensus; SSZ'16 attack space; α=0.33 attacker\n",
      "public_blocks: 2\n",
      "private_blocks: 1\n",
      "diff_blocks: -1\n",
      "event: `ProofOfWork\n",
      "Actions: (0) Adopt | (1) Override | (2) Match | (3) Wait\n",
      "\n",
      "Tailstorm with k=8, discount rewards, and heuristic sub-block selection; SSZ'16-like attack space; α=0.45 attacker\n",
      "public_blocks: 1\n",
      "private_blocks: 1\n",
      "diff_blocks: 0\n",
      "public_votes: 2\n",
      "private_votes_inclusive: 1\n",
      "private_votes_exclusive: 1\n",
      "public_depth: 1\n",
      "private_depth_inclusive: 0\n",
      "private_depth_exclusive: 0\n",
      "event: `Network\n",
      "Actions: (0) Adopt_Prolong | (1) Override_Prolong | (2) Match_Prolong | (3) Wait_Prolong | (4) Adopt_Proceed | (5) Override_Proceed | (6) Match_Proceed | (7) Wait_Proceed\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import gym\n",
    "\n",
    "\n",
    "def env_fn(config={}):\n",
    "    protocol_fn = getattr(cpr_gym.protocols, config.get(\"protocol\", \"nakamoto\"))\n",
    "    protocol_args = config.get(\"protocol_args\", {})\n",
    "\n",
    "    episode_len = config.get(\"episode_len\", 128)\n",
    "\n",
    "    alpha = config.get(\"alpha\", 0.33)\n",
    "    gamma = config.get(\"gamma\", 0.5)\n",
    "    if \"defenders\" in config:\n",
    "        defenders = config[\"defenders\"]\n",
    "    else:\n",
    "        defenders = math.ceil((1 - alpha) / (1 - gamma))\n",
    "\n",
    "    rewards = dict(\n",
    "        sparse_relative=(\n",
    "            cpr_gym.wrappers.SparseRelativeRewardWrapper,\n",
    "            dict(max_steps=episode_len),\n",
    "        ),\n",
    "        sparse_per_progress=(\n",
    "            cpr_gym.wrappers.SparseRewardPerProgressWrapper,\n",
    "            dict(max_steps=episode_len),\n",
    "        ),\n",
    "        dense_per_progress=(\n",
    "            lambda env: cpr_gym.wrappers.DenseRewardPerProgressWrapper(\n",
    "                env, episode_len=episode_len\n",
    "            ),\n",
    "            dict(),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    reward_wrapper, env_args = rewards[config.get(\"reward\", \"sparse_relative\")]\n",
    "\n",
    "    env = gym.make(\n",
    "        \"cpr_gym:core-v0\",\n",
    "        proto=protocol_fn(**protocol_args),\n",
    "        alpha=alpha,\n",
    "        gamma=gamma,\n",
    "        defenders=defenders,\n",
    "        **env_args\n",
    "    )\n",
    "\n",
    "    env = reward_wrapper(env)\n",
    "\n",
    "    env = cpr_gym.wrappers.ClearInfoWrapper(env)\n",
    "\n",
    "    return env\n",
    "\n",
    "\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "test_env = env_fn()\n",
    "check_env(test_env)\n",
    "test_env.render()\n",
    "\n",
    "print()\n",
    "test_env = env_fn(\n",
    "    dict(\n",
    "        alpha=0.45,\n",
    "        protocol=\"tailstorm\",\n",
    "        protocol_args=dict(k=8, reward=\"discount\", subblock_selection=\"heuristic\"),\n",
    "    )\n",
    ")\n",
    "check_env(test_env)\n",
    "test_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eacce0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f15c3f03040>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import stable_baselines3\n",
    "import torch\n",
    "\n",
    "\n",
    "def ppo(config={}):\n",
    "    env_config = config.get(\"env_config\", {})\n",
    "\n",
    "    def configured_env_fn():\n",
    "        return env_fn(env_config)\n",
    "\n",
    "    # Vectorize\n",
    "    n_vec = config.get(\"vectorize\", 1)\n",
    "    if n_vec > 1 and not config.get(\"vectorize_dummy\", False):\n",
    "        env = stable_baselines3.common.vec_env.SubprocVecEnv(\n",
    "            [configured_env_fn] * n_vec\n",
    "        )\n",
    "    else:\n",
    "        env = stable_baselines3.common.vec_env.DummyVecEnv([configured_env_fn] * n_vec)\n",
    "\n",
    "    # Monitor\n",
    "    env = stable_baselines3.common.vec_env.VecMonitor(env)\n",
    "\n",
    "    # Learning-rate schedule\n",
    "    lr_start = config.get(\"lr_start\", 1e-3)\n",
    "    lr_end = max(lr_start, config.get(\"lr_end\", 1e-5))\n",
    "\n",
    "    def lr_schedule(remaining):\n",
    "        return lr_start * remaining + lr_end * (1 - remaining)\n",
    "\n",
    "    batch_size = config.get(\"batch_size\", 1024)\n",
    "    n_steps_multiple = config.get(\"n_steps_multiple\", 1)\n",
    "    layer_size = config.get(\"layer_size\", 64)\n",
    "    n_layers = config.get(\"n_layers\", 3)\n",
    "    model = stable_baselines3.PPO(\n",
    "        \"MlpPolicy\",\n",
    "        env=env,\n",
    "        verbose=config.get(\"verbose\", 0),\n",
    "        batch_size=batch_size,\n",
    "        gamma=config.get(\"gamma\", 0.999),\n",
    "        n_steps=batch_size * n_steps_multiple,\n",
    "        clip_range=config.get(\"clip_range\", 0.1),\n",
    "        learning_rate=lr_schedule,\n",
    "        policy_kwargs=dict(\n",
    "            activation_fn=torch.nn.ReLU,\n",
    "            net_arch=[\n",
    "                dict(\n",
    "                    pi=[int(layer_size)] * n_layers,\n",
    "                    vf=[int(layer_size)] * n_layers,\n",
    "                )\n",
    "            ],\n",
    "        ),\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "ppo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "416f6484",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import skopt\n",
    "import time\n",
    "\n",
    "space = dict(\n",
    "    lr_start=skopt.space.Real(\n",
    "        name=\"lr_start\", low=1e-5, high=1e-2, prior=\"log-uniform\"\n",
    "    ),\n",
    "    lr_decay=skopt.space.Real(name=\"lr_decay\", low=1e-3, high=1, prior=\"log-uniform\"),\n",
    "    clip_range=skopt.space.Real(name=\"clip_range\", low=0.01, high=0.5, prior=\"uniform\"),\n",
    "    gamma=skopt.space.Real(name=\"gamma\", low=0.9, high=1, prior=\"log-uniform\"),\n",
    "    n_vec=skopt.space.Integer(name=\"n_vec\", low=6, high=12, prior=\"uniform\"),\n",
    "    batch_size=skopt.space.Integer(\n",
    "        name=\"batch_size\", low=128, high=8192, prior=\"log-uniform\"\n",
    "    ),\n",
    "    n_steps_multiple=skopt.space.Integer(\n",
    "        name=\"n_steps_multiple\", low=1, high=4, prior=\"uniform\"\n",
    "    ),\n",
    "    layer_size=skopt.space.Categorical([8, 16, 32, 64, 128], name=\"layer_size\"),\n",
    "    n_layers=skopt.space.Integer(name=\"n_layers\", low=2, high=4, prior=\"uniform\"),\n",
    ")\n",
    "dimensions = skopt.utils.dimensions_aslist(space)\n",
    "\n",
    "env_config = dict(\n",
    "    alpha=0.45,\n",
    "    gamma=0.5,\n",
    "    reward=\"sparse_relative\",\n",
    "    episode_len=128,\n",
    ")\n",
    "\n",
    "\n",
    "eval_n_episodes = 16\n",
    "training_budget_steps = 5e5\n",
    "\n",
    "\n",
    "@skopt.utils.use_named_args(dimensions=dimensions)\n",
    "def objective(\n",
    "    lr_start,\n",
    "    lr_decay,\n",
    "    n_vec,\n",
    "    batch_size,\n",
    "    n_steps_multiple,\n",
    "    layer_size,\n",
    "    n_layers,\n",
    "    clip_range,\n",
    "    gamma,\n",
    "):\n",
    "    alpha = env_config[\"alpha\"]\n",
    "\n",
    "    config = dict(\n",
    "        env_config=env_config,\n",
    "        lr_start=lr_start,\n",
    "        lr_end=lr_start * lr_decay,\n",
    "        vectorize=n_vec,\n",
    "        batch_size=batch_size,\n",
    "        n_steps_multiple=n_steps_multiple,\n",
    "        layer_size=layer_size,\n",
    "        n_layers=n_layers,\n",
    "        clip_range=clip_range,\n",
    "        gamma=gamma,\n",
    "    )\n",
    "\n",
    "    log = dict(\n",
    "        config=config,\n",
    "        time=dict(),\n",
    "        eval_n_episodes=eval_n_episodes,\n",
    "        training_budget_steps=training_budget_steps,\n",
    "    )\n",
    "\n",
    "    model = ppo(config)\n",
    "\n",
    "    # Train\n",
    "    start_training = time.time()\n",
    "    model.learn(total_timesteps=training_budget_steps)\n",
    "    log[\"time\"][\"training_s\"] = time.time() - start_training\n",
    "\n",
    "    # Eval\n",
    "    start_eval = time.time()\n",
    "    env = env_fn()\n",
    "    obs = env.reset()\n",
    "    rewards = []\n",
    "    i = 0\n",
    "    while i < eval_n_episodes:\n",
    "        i += 1\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        reward = 0\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, r, done, info = env.step(action)\n",
    "            reward += r\n",
    "        reward = reward / alpha - 1\n",
    "        rewards.append(reward)\n",
    "\n",
    "    log[\"time\"][\"eval_s\"] = time.time() - start_eval\n",
    "\n",
    "    mean_reward = np.mean(rewards)\n",
    "    log[\"mean_reward\"] = mean_reward\n",
    "\n",
    "    del model\n",
    "\n",
    "    return log\n",
    "\n",
    "\n",
    "def point(**kwargs):\n",
    "    return skopt.utils.point_aslist(space, dict(**kwargs))\n",
    "\n",
    "\n",
    "# objective(point(lr=1e-3, n_vec=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5032ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skopt.plots\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display\n",
    "import datetime\n",
    "from collections import deque\n",
    "\n",
    "optimizer = skopt.Optimizer(dimensions)\n",
    "\n",
    "next_x = optimizer.ask()\n",
    "last_r = -1.0\n",
    "max_x = next_x\n",
    "max_r = last_r\n",
    "log = []\n",
    "times = deque([], 10)\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29e84078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'config': {'env_config': {'alpha': 0.45,\n",
       "   'gamma': 0.5,\n",
       "   'reward': 'sparse_relative',\n",
       "   'episode_len': 128},\n",
       "  'lr_start': 0.001,\n",
       "  'lr_end': 0.001,\n",
       "  'vectorize': 1,\n",
       "  'batch_size': 1024,\n",
       "  'n_steps_multiple': 1,\n",
       "  'layer_size': 16,\n",
       "  'n_layers': 3,\n",
       "  'clip_range': 0.1,\n",
       "  'gamma': 0.999},\n",
       " 'time': {'training_s': 177.15141940116882, 'eval_s': 0.46401000022888184},\n",
       " 'eval_n_episodes': 16,\n",
       " 'training_budget_steps': 500000.0,\n",
       " 'mean_reward': -0.3005503266793349}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = point(\n",
    "    lr_start=1e-3,\n",
    "    lr_decay=1,\n",
    "    clip_range=0.1,\n",
    "    n_vec=1,\n",
    "    batch_size=1024,\n",
    "    gamma=0.999,\n",
    "    n_layers=3,\n",
    "    layer_size=16,\n",
    "    n_steps_multiple=1,\n",
    ")\n",
    "objective(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82057a39",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-18 08:10:48.541088\n",
      "iteration 521\n",
      "28 iterations per hour\n",
      "\n",
      "last r: -0.3409806410946399\n",
      "next x: {'batch_size': 128, 'clip_range': 0.10812895857629526, 'gamma': 1.0, 'layer_size': 128, 'lr_decay': 1.0, 'lr_start': 0.0002882052349594924, 'n_layers': 4, 'n_steps_multiple': 1, 'n_vec': 6}\n",
      "\n",
      "best r: -0.11839819027674037\n",
      "best x: {'batch_size': 404, 'clip_range': 0.01, 'gamma': 1.0, 'layer_size': 128, 'lr_decay': 0.007729720805333112, 'lr_start': 0.0017582507769772183, 'n_layers': 4, 'n_steps_multiple': 1, 'n_vec': 9}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest x: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mdict\u001b[39m(skopt\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mpoint_asdict(space, max_x))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 16\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m times\u001b[38;5;241m.\u001b[39mappend(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start)\n\u001b[1;32m     18\u001b[0m log\u001b[38;5;241m.\u001b[39mappend((next_x, ret))\n",
      "File \u001b[0;32m~/devel/cpr2/_venv/lib64/python3.9/site-packages/skopt/utils.py:789\u001b[0m, in \u001b[0;36muse_named_args.<locals>.decorator.<locals>.wrapper\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    786\u001b[0m arg_dict \u001b[38;5;241m=\u001b[39m {dim\u001b[38;5;241m.\u001b[39mname: value \u001b[38;5;28;01mfor\u001b[39;00m dim, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(dimensions, x)}\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Call the wrapped objective function with the named arguments.\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m objective_value \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marg_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m objective_value\n",
      "Cell \u001b[0;32mIn [4], line 74\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(lr_start, lr_decay, n_vec, batch_size, n_steps_multiple, layer_size, n_layers, clip_range, gamma)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m     73\u001b[0m start_training \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 74\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_budget_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m log[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_s\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_training\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Eval\u001b[39;00m\n",
      "File \u001b[0;32m~/devel/cpr2/_venv/lib64/python3.9/site-packages/stable_baselines3/ppo/ppo.py:317\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28mself\u001b[39m: PPOSelf,\n\u001b[1;32m    305\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    315\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PPOSelf:\n\u001b[0;32m--> 317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_log_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_log_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/devel/cpr2/_venv/lib64/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py:283\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mrecord(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime/total_timesteps\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps, exclude\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboard\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdump(step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps)\n\u001b[0;32m--> 283\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/devel/cpr2/_venv/lib64/python3.9/site-packages/stable_baselines3/ppo/ppo.py:276\u001b[0m, in \u001b[0;36mPPO.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;66;03m# Optimization step\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 276\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;66;03m# Clip grad norm\u001b[39;00m\n\u001b[1;32m    278\u001b[0m th\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_grad_norm)\n",
      "File \u001b[0;32m~/devel/cpr2/_venv/lib64/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/devel/cpr2/_venv/lib64/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# for _ in range(10):\n",
    "while True:\n",
    "    i += 1\n",
    "    IPython.display.clear_output()\n",
    "    print(str(datetime.datetime.now()))\n",
    "    print(f\"iteration {i}\")\n",
    "    print(f\"{3600 / np.mean(times):1.0f} iterations per hour\")\n",
    "    print()\n",
    "    print(f\"last r: {last_r}\")\n",
    "    print(f\"next x: {dict(skopt.utils.point_asdict(space, next_x))}\")\n",
    "    print()\n",
    "    print(f\"best r: {max_r}\")\n",
    "    print(f\"best x: {dict(skopt.utils.point_asdict(space, max_x))}\")\n",
    "\n",
    "    start = time.time()\n",
    "    ret = objective(next_x)\n",
    "    times.append(time.time() - start)\n",
    "    log.append((next_x, ret))\n",
    "\n",
    "    last_r = ret[\"mean_reward\"]\n",
    "    if last_r > max_r:\n",
    "        max_r = last_r\n",
    "        max_x = next_x\n",
    "    optimizer.tell(next_x, -last_r)\n",
    "    next_x = optimizer.ask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d2e121",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
