{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "293f26ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local-v0.5.4-2-ga05e3ef'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cpr_gym\n",
    "\n",
    "cpr_gym.engine.cpr_lib_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bf122ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nakamoto consensus; SSZ'16 attack space; α=0.33 attacker\n",
      "public_blocks: 2\n",
      "private_blocks: 2\n",
      "diff_blocks: 0\n",
      "event: 1\n",
      "Actions: (0) Adopt | (1) Override | (2) Match | (3) Wait\n",
      "\n",
      "Tailstorm with k=8, discount rewards, and heuristic sub-block selection; SSZ'16-like attack space; α=0.45 attacker\n",
      "public_blocks: 1\n",
      "private_blocks: 1\n",
      "diff_blocks: 0\n",
      "public_votes: 2\n",
      "private_votes_inclusive: 1\n",
      "private_votes_exclusive: 1\n",
      "public_depth: 1\n",
      "private_depth_inclusive: 0\n",
      "private_depth_exclusive: 0\n",
      "event: 1\n",
      "Actions: (0) Adopt_Prolong | (1) Override_Prolong | (2) Match_Prolong | (3) Wait_Prolong | (4) Adopt_Proceed | (5) Override_Proceed | (6) Match_Proceed | (7) Wait_Proceed\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import gym\n",
    "\n",
    "\n",
    "def env_fn(config={}):\n",
    "    protocol_fn = getattr(cpr_gym.protocols, config.get(\"protocol\", \"nakamoto\"))\n",
    "    protocol_args = config.get(\"protocol_args\", {})\n",
    "\n",
    "    episode_len = config.get(\"episode_len\", 128)\n",
    "\n",
    "    alpha = config.get(\"alpha\", 0.33)\n",
    "    gamma = config.get(\"gamma\", 0.5)\n",
    "    if \"defenders\" in config:\n",
    "        defenders = config[\"defenders\"]\n",
    "    else:\n",
    "        defenders = math.ceil((1 - alpha) / (1 - gamma))\n",
    "\n",
    "    rewards = dict(\n",
    "        sparse_relative=(\n",
    "            cpr_gym.wrappers.SparseRelativeRewardWrapper,\n",
    "            dict(max_steps=episode_len),\n",
    "        ),\n",
    "        sparse_per_progress=(\n",
    "            cpr_gym.wrappers.SparseRewardPerProgressWrapper,\n",
    "            dict(max_steps=episode_len),\n",
    "        ),\n",
    "        dense_per_progress=(\n",
    "            lambda env: cpr_gym.wrappers.DenseRewardPerProgressWrapper(\n",
    "                env, episode_len=episode_len\n",
    "            ),\n",
    "            dict(),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    reward_wrapper, env_args = rewards[config.get(\"reward\", \"sparse_relative\")]\n",
    "\n",
    "    env = gym.make(\n",
    "        \"cpr_gym:core-v0\",\n",
    "        proto=protocol_fn(**protocol_args),\n",
    "        alpha=alpha,\n",
    "        gamma=gamma,\n",
    "        defenders=defenders,\n",
    "        **env_args\n",
    "    )\n",
    "\n",
    "    env = reward_wrapper(env)\n",
    "\n",
    "    env = cpr_gym.wrappers.ClearInfoWrapper(env)\n",
    "\n",
    "    return env\n",
    "\n",
    "\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "test_env = env_fn()\n",
    "check_env(test_env)\n",
    "test_env.render()\n",
    "\n",
    "print()\n",
    "test_env = env_fn(\n",
    "    dict(\n",
    "        alpha=0.45,\n",
    "        protocol=\"tailstorm\",\n",
    "        protocol_args=dict(k=8, reward=\"discount\", subblock_selection=\"heuristic\"),\n",
    "    )\n",
    ")\n",
    "check_env(test_env)\n",
    "test_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eacce0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f232f39f7f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import stable_baselines3\n",
    "import torch\n",
    "\n",
    "\n",
    "def ppo(config={}):\n",
    "    env_config = config.get(\"env_config\", {})\n",
    "\n",
    "    def configured_env_fn():\n",
    "        return env_fn(env_config)\n",
    "\n",
    "    # Vectorize\n",
    "    n_vec = config.get(\"vectorize\", 1)\n",
    "    if n_vec > 1 and not config.get(\"vectorize_dummy\", False):\n",
    "        env = stable_baselines3.common.vec_env.SubprocVecEnv(\n",
    "            [configured_env_fn] * n_vec\n",
    "        )\n",
    "    else:\n",
    "        env = stable_baselines3.common.vec_env.DummyVecEnv([configured_env_fn] * n_vec)\n",
    "\n",
    "    # Monitor\n",
    "    env = stable_baselines3.common.vec_env.VecMonitor(env)\n",
    "\n",
    "    # Learning-rate schedule\n",
    "    lr_start = config.get(\"lr_start\", 1e-3)\n",
    "    lr_end = max(lr_start, config.get(\"lr_end\", 1e-5))\n",
    "\n",
    "    def lr_schedule(remaining):\n",
    "        return lr_start * remaining + lr_end * (1 - remaining)\n",
    "\n",
    "    batch_size = config.get(\"batch_size\", 1024)\n",
    "    n_steps_multiple = config.get(\"n_steps_multiple\", 1)\n",
    "    layer_size = config.get(\"layer_size\", 64)\n",
    "    n_layers = config.get(\"n_layers\", 3)\n",
    "    model = stable_baselines3.PPO(\n",
    "        \"MlpPolicy\",\n",
    "        env=env,\n",
    "        verbose=config.get(\"verbose\", 0),\n",
    "        batch_size=batch_size,\n",
    "        gamma=config.get(\"gamma\", 0.999),\n",
    "        n_steps=batch_size * n_steps_multiple,\n",
    "        clip_range=config.get(\"clip_range\", 0.1),\n",
    "        learning_rate=lr_schedule,\n",
    "        policy_kwargs=dict(\n",
    "            activation_fn=torch.nn.ReLU,\n",
    "            net_arch=[\n",
    "                dict(\n",
    "                    pi=[int(layer_size)] * n_layers,\n",
    "                    vf=[int(layer_size)] * n_layers,\n",
    "                )\n",
    "            ],\n",
    "        ),\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "ppo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "416f6484",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import skopt\n",
    "import time\n",
    "\n",
    "space = dict(\n",
    "    lr_start=skopt.space.Real(\n",
    "        name=\"lr_start\", low=1e-5, high=1e-2, prior=\"log-uniform\"\n",
    "    ),\n",
    "    lr_decay=skopt.space.Real(name=\"lr_decay\", low=1e-3, high=1, prior=\"log-uniform\"),\n",
    "    clip_range=skopt.space.Real(name=\"clip_range\", low=0.01, high=0.5, prior=\"uniform\"),\n",
    "    gamma=skopt.space.Real(name=\"gamma\", low=0.9, high=1, prior=\"log-uniform\"),\n",
    "    n_vec=skopt.space.Integer(name=\"n_vec\", low=6, high=12, prior=\"uniform\"),\n",
    "    batch_size=skopt.space.Integer(\n",
    "        name=\"batch_size\", low=128, high=8192, prior=\"log-uniform\"\n",
    "    ),\n",
    "    n_steps_multiple=skopt.space.Integer(\n",
    "        name=\"n_steps_multiple\", low=1, high=4, prior=\"uniform\"\n",
    "    ),\n",
    "    layer_size=skopt.space.Categorical([8, 16, 32, 64, 128], name=\"layer_size\"),\n",
    "    n_layers=skopt.space.Integer(name=\"n_layers\", low=2, high=4, prior=\"uniform\"),\n",
    ")\n",
    "dimensions = skopt.utils.dimensions_aslist(space)\n",
    "\n",
    "env_config = dict(\n",
    "    alpha=0.45,\n",
    "    gamma=0.5,\n",
    "    reward=\"sparse_relative\",\n",
    "    episode_len=128,\n",
    ")\n",
    "\n",
    "\n",
    "eval_n_episodes = 16\n",
    "training_budget_steps = 5e5\n",
    "\n",
    "\n",
    "@skopt.utils.use_named_args(dimensions=dimensions)\n",
    "def objective(\n",
    "    lr_start,\n",
    "    lr_decay,\n",
    "    n_vec,\n",
    "    batch_size,\n",
    "    n_steps_multiple,\n",
    "    layer_size,\n",
    "    n_layers,\n",
    "    clip_range,\n",
    "    gamma,\n",
    "):\n",
    "    alpha = env_config[\"alpha\"]\n",
    "\n",
    "    config = dict(\n",
    "        env_config=env_config,\n",
    "        lr_start=lr_start,\n",
    "        lr_end=lr_start * lr_decay,\n",
    "        vectorize=n_vec,\n",
    "        batch_size=batch_size,\n",
    "        n_steps_multiple=n_steps_multiple,\n",
    "        layer_size=layer_size,\n",
    "        n_layers=n_layers,\n",
    "        clip_range=clip_range,\n",
    "        gamma=gamma,\n",
    "    )\n",
    "\n",
    "    log = dict(\n",
    "        config=config,\n",
    "        time=dict(),\n",
    "        eval_n_episodes=eval_n_episodes,\n",
    "        training_budget_steps=training_budget_steps,\n",
    "    )\n",
    "\n",
    "    model = ppo(config)\n",
    "\n",
    "    # Train\n",
    "    start_training = time.time()\n",
    "    model.learn(total_timesteps=training_budget_steps)\n",
    "    log[\"time\"][\"training_s\"] = time.time() - start_training\n",
    "\n",
    "    # Eval\n",
    "    start_eval = time.time()\n",
    "    env = env_fn()\n",
    "    obs = env.reset()\n",
    "    rewards = []\n",
    "    i = 0\n",
    "    while i < eval_n_episodes:\n",
    "        i += 1\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        reward = 0\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, r, done, info = env.step(action)\n",
    "            reward += r\n",
    "        reward = reward / alpha - 1\n",
    "        rewards.append(reward)\n",
    "\n",
    "    log[\"time\"][\"eval_s\"] = time.time() - start_eval\n",
    "\n",
    "    mean_reward = np.mean(rewards)\n",
    "    log[\"mean_reward\"] = mean_reward\n",
    "\n",
    "    del model\n",
    "\n",
    "    return log\n",
    "\n",
    "\n",
    "def point(**kwargs):\n",
    "    return skopt.utils.point_aslist(space, dict(**kwargs))\n",
    "\n",
    "\n",
    "# objective(point(lr=1e-3, n_vec=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5032ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skopt.plots\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display\n",
    "import datetime\n",
    "from collections import deque\n",
    "\n",
    "optimizer = skopt.Optimizer(dimensions)\n",
    "\n",
    "next_x = optimizer.ask()\n",
    "last_r = -1.0\n",
    "max_x = next_x\n",
    "max_r = last_r\n",
    "log = []\n",
    "times = deque([], 10)\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82057a39",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-17 16:13:06.319742\n",
      "iteration 2\n",
      "51 iterations per hour\n",
      "\n",
      "last r: -0.8664551346146501\n",
      "next x: {'batch_size': 152, 'clip_range': 0.2513391168311672, 'gamma': 0.985429920998278, 'layer_size': 32, 'lr_decay': 0.003045464095242069, 'lr_start': 2.2122112370905427e-05, 'n_layers': 2, 'n_steps_multiple': 3, 'n_vec': 8}\n",
      "\n",
      "best r: -0.8664551346146501\n",
      "best x: {'batch_size': 3658, 'clip_range': 0.18732901975499391, 'gamma': 0.9724592123961149, 'layer_size': 64, 'lr_decay': 0.0634034429154054, 'lr_start': 0.00010168680903921187, 'n_layers': 3, 'n_steps_multiple': 3, 'n_vec': 11}\n"
     ]
    }
   ],
   "source": [
    "# for _ in range(10):\n",
    "while True:\n",
    "    i += 1\n",
    "    IPython.display.clear_output()\n",
    "    print(str(datetime.datetime.now()))\n",
    "    print(f\"iteration {i}\")\n",
    "    print(f\"{3600 / np.mean(times):1.0f} iterations per hour\")\n",
    "    print()\n",
    "    print(f\"last r: {last_r}\")\n",
    "    print(f\"next x: {dict(skopt.utils.point_asdict(space, next_x))}\")\n",
    "    print()\n",
    "    print(f\"best r: {max_r}\")\n",
    "    print(f\"best x: {dict(skopt.utils.point_asdict(space, max_x))}\")\n",
    "\n",
    "    start = time.time()\n",
    "    ret = objective(next_x)\n",
    "    times.append(time.time() - start)\n",
    "    log.append((next_x, ret))\n",
    "\n",
    "    last_r = ret[\"mean_reward\"]\n",
    "    if last_r > max_r:\n",
    "        max_r = last_r\n",
    "        max_x = next_x\n",
    "    optimizer.tell(next_x, -last_r)\n",
    "    next_x = optimizer.ask()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
