[wandb]
# tags = list, your, tags, here

[main]
env = cpr_gym:core-v0
protocol = nakamoto
# protocol = bk
# protocol = bkll
# protocol = tailstorm
# alpha = .33
alpha_min = .1
alpha_max = .5
# alpha_schedule = .1, .2, .33, .44
# reward = sparse_relative
# reward = sparse_per_block
# reward = dense_per_block
episode_len = 64
total_timesteps = 1e8
# n_envs = psutil.num_cpu()

[env_args]
activation_delay = 1
defenders = 1
gamma = 0

[eval]
; If we train on single alpha or list of alphas we evaluate on all alphas.
; If we train on a range of alphas we use alpha_step to derive a list of
; alphas to evaluate on.
alpha_step = 0.01

; How many episodes per evaluation per n_env per alpha?
episodes_per_alpha_per_env = 5

; For reporting, we maintain a ring buffer of past episodes in each n_env.
; When reporting per_alpha statistics, we calculate the mean over this ring
; buffer. The number of recorded episodes is
; episodes_per_alpha_per_env * # alphas * n_envs * recorder_multiple
recorder_multiple = 25

; Reporting per_alpha statistics for all alphas might overload the dashboard.
; Set report_alpha = n to report only every n-th alpha.
report_alpha = 5

[protocol_args]
; Set number of puzzles per block for bk, bkll, and tailstorm.
# k = 8

; The default reward scheme is constant reward per confirmed puzzle solution.
reward = constant

; Tailstorm proposes to discount the reward based on the depth of the sub block
; tree
# reward = discount

[ppo]
batch_size = 2048
n_steps_multiple = 10
layer_size = 100
n_layers = 2
gamma = 0.999
starting_lr = 1e-5
ending_lr = 1e-6
