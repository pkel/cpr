{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37877d6d",
   "metadata": {},
   "source": [
    "We have two gym wrappers that calculate sparse rewards at the end of the episode.\n",
    "\n",
    "**relativeReward** calculates sum of attacker reward divided by sum of all rewards.\n",
    "\n",
    "**rewardPerBlock** calculates sum of attacker reward divided by number of proof-of-work puzzle solutions in the common chain.\n",
    "\n",
    "Hypothesis 1: The wrappers are equivalant for protocols that hand out a constant reward per confirmed puzzle solutions. The canonical example is Bitcoin/Nakamoto, but one could also run Tailstorm on such an incentive scheme.\n",
    "\n",
    "Hypothesis 2: The **rewardPerBlock** wrapper captures the effect of Tailstorm's 'discount' reward scheme, where some puzzle solutions get smaller rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91c0ce48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cpr_gym import protocols, wrappers\n",
    "import itertools\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c4694a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_fn(alpha, proto=protocols.nakamoto(), steps=2016):\n",
    "    env = gym.make(\n",
    "        \"cpr_gym:core-v0\", proto=proto, alpha=alpha, activation_delay=1, max_steps=steps\n",
    "    )\n",
    "    return env\n",
    "\n",
    "\n",
    "def episode(env, policy=\"honest\"):\n",
    "    p = lambda obs: env.policy(obs, policy)\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        obs, reward, done, info = env.step(p(obs))\n",
    "    return reward\n",
    "\n",
    "\n",
    "def measure(\n",
    "    alpha,\n",
    "    proto=protocols.nakamoto(),\n",
    "    policy=\"honest\",\n",
    "    steps_per_episode=2000,\n",
    "    steps=5000,\n",
    "    wrapper=lambda x: x,\n",
    "):\n",
    "    env = wrapper(env_fn(alpha, proto=proto, steps=steps_per_episode))\n",
    "    # run measurements\n",
    "    reward = []\n",
    "    for i in range(int(np.ceil(steps / steps_per_episode))):\n",
    "        try:\n",
    "            r = episode(env, policy)\n",
    "            reward.append(r)\n",
    "        except ValueError as e:\n",
    "            warnings.warn(e)\n",
    "    return np.mean(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d860928",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mmeasure\u001b[0;34m(alpha, proto, policy, steps_per_episode, steps, wrapper)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 29\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mepisode\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     reward\u001b[38;5;241m.\u001b[39mappend(r)\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mepisode\u001b[0;34m(env, policy)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m---> 13\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reward\n",
      "File \u001b[0;32m~/devel/cpr/python/gym/cpr_gym/wrappers.py:21\u001b[0m, in \u001b[0;36mSparseRelativeRewardWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m---> 21\u001b[0m     obs, _reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrrw_def \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreward_defender\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/devel/cpr/_venv/lib64/python3.9/site-packages/gym/wrappers/order_enforcing.py:11\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m observation, reward, done, info\n",
      "File \u001b[0;32m~/devel/cpr/python/gym/cpr_gym/envs.py:43\u001b[0m, in \u001b[0;36mCore.step\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, a):\n\u001b[0;32m---> 43\u001b[0m     obs, r, d, i \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mocaml_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     obs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(obs)  \u001b[38;5;66;03m# for pickling; why doesn't pyml support pickling?\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: ocaml error \"Assert_failure ocaml/gym/engine.ml:304:4\"\nRaised at Dune__exe__Engine.of_module.step in file \"ocaml/gym/engine.ml\", line 304, characters 4-52\nCalled from Dune__exe__Bridge.(fun) in file \"ocaml/gym/bridge.ml\", line 100, characters 27-43\nCalled from Python_lib__Defunc.apply.loop in file \"src/defunc.ml\", line 123, characters 8-11\nCalled from Python_lib__Defunc.apply in file \"src/defunc.ml\", line 179, characters 23-47\nCalled from Python_lib__Py_module.wrap_ocaml_errors in file \"src/py_module.ml\", line 38, characters 6-10\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     env \u001b[38;5;241m=\u001b[39m env_fn(\u001b[38;5;241m0.33\u001b[39m, proto\u001b[38;5;241m=\u001b[39mproto)\n\u001b[1;32m     15\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m     16\u001b[0m         itertools\u001b[38;5;241m.\u001b[39mproduct([proto_s], alpha, spe, wrap, env\u001b[38;5;241m.\u001b[39mpolicies()),\n\u001b[1;32m     17\u001b[0m         columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprotocol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps_per_episode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrapper\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     18\u001b[0m     )\n\u001b[0;32m---> 19\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeasure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m            \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprotos\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprotocol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m            \u001b[49m\u001b[43msteps_per_episode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps_per_episode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwrapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrap\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrapper\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     dfs\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[1;32m     30\u001b[0m d \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(dfs, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/devel/cpr/_venv/lib64/python3.9/site-packages/pandas/core/frame.py:8839\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   8828\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m   8830\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m   8831\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   8832\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   8837\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m   8838\u001b[0m )\n\u001b[0;32m-> 8839\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/devel/cpr/_venv/lib64/python3.9/site-packages/pandas/core/apply.py:727\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    725\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[0;32m--> 727\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/devel/cpr/_venv/lib64/python3.9/site-packages/pandas/core/apply.py:851\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 851\u001b[0m     results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m~/devel/cpr/_venv/lib64/python3.9/site-packages/pandas/core/apply.py:867\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    865\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m    866\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 867\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    868\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    869\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    870\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    871\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     14\u001b[0m     env \u001b[38;5;241m=\u001b[39m env_fn(\u001b[38;5;241m0.33\u001b[39m, proto\u001b[38;5;241m=\u001b[39mproto)\n\u001b[1;32m     15\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m     16\u001b[0m         itertools\u001b[38;5;241m.\u001b[39mproduct([proto_s], alpha, spe, wrap, env\u001b[38;5;241m.\u001b[39mpolicies()),\n\u001b[1;32m     17\u001b[0m         columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprotocol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps_per_episode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrapper\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     18\u001b[0m     )\n\u001b[1;32m     19\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m---> 20\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mmeasure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m            \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprotos\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprotocol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m            \u001b[49m\u001b[43msteps_per_episode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps_per_episode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwrapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrap\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrapper\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     27\u001b[0m         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     28\u001b[0m     )\n\u001b[1;32m     29\u001b[0m     dfs\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[1;32m     30\u001b[0m d \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(dfs, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mmeasure\u001b[0;34m(alpha, proto, policy, steps_per_episode, steps, wrapper)\u001b[0m\n\u001b[1;32m     30\u001b[0m         reward\u001b[38;5;241m.\u001b[39mappend(r)\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 32\u001b[0m         \u001b[43mwarnings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarn\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(reward)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "alpha = np.arange(0.1, 1.05, 0.1)\n",
    "spe = [8, 16, 32, 64, 128]\n",
    "wrap = dict(\n",
    "    sparseRelativeReward=wrappers.SparseRelativeRewardWrapper,\n",
    "    sparseRewardPerBlock=wrappers.SparseRewardPerBlockWrapper,\n",
    ")\n",
    "protos = dict(\n",
    "    nakamoto=protocols.nakamoto(),\n",
    "    tailstorm8constant=protocols.tailstorm(k=8, reward=\"constant\"),\n",
    "    tailstorm8discount=protocols.tailstorm(k=8, reward=\"discount\"),\n",
    ")\n",
    "dfs = []\n",
    "for proto_s, proto in protos.items():\n",
    "    env = env_fn(0.33, proto=proto)\n",
    "    df = pd.DataFrame(\n",
    "        itertools.product([proto_s], alpha, spe, wrap, env.policies()),\n",
    "        columns=[\"protocol\", \"alpha\", \"steps_per_episode\", \"wrapper\", \"policy\"],\n",
    "    )\n",
    "    df[\"reward\"] = df.apply(\n",
    "        lambda x: measure(\n",
    "            x.alpha,\n",
    "            proto=protos[x.protocol],\n",
    "            policy=x.policy,\n",
    "            steps_per_episode=x.steps_per_episode,\n",
    "            wrapper=wrap[x.wrapper],\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "    dfs.append(df)\n",
    "d = pd.concat(dfs, ignore_index=True)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9887464",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.relplot(\n",
    "    data=d,\n",
    "    kind=\"line\",\n",
    "    x=\"alpha\",\n",
    "    y=\"reward\",\n",
    "    hue=\"policy\",\n",
    "    style=\"wrapper\",\n",
    "    col=\"steps_per_episode\",\n",
    "    row=\"protocol\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175c48fe",
   "metadata": {},
   "source": [
    "This plot confirms Hypothesis 1. For Nakamoto and Tailstorm with constant reward, both wrappers produce the same results.\n",
    "\n",
    "The plot supports Hypotheses 2. For Tailstorm with discount reward, the **rewardPerBlock** wrapper produces smaller rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7c4e09",
   "metadata": {},
   "source": [
    "---\n",
    "The following plots are for comparing the effect of steps_per_second on Nakamoto. It seems that we have to train on steps_per_second greater equal 16, in order to discover selfish mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230c8f4d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.relplot(\n",
    "    data=d[d.protocol == \"nakamoto\"],\n",
    "    kind=\"line\",\n",
    "    x=\"alpha\",\n",
    "    y=\"reward\",\n",
    "    hue=\"policy\",\n",
    "    col=\"wrapper\",\n",
    "    row=\"steps_per_episode\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358aa9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(\n",
    "    data=d[d.protocol == \"nakamoto\"],\n",
    "    kind=\"line\",\n",
    "    x=\"alpha\",\n",
    "    y=\"reward\",\n",
    "    hue=\"steps_per_episode\",\n",
    "    row=\"policy\",\n",
    "    col=\"wrapper\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
