{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "024cda7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import barzur20aft\n",
    "import compiler\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ab5a2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitcoin_mdp(*args, **kwargs):\n",
    "    model = barzur20aft.Bitcoin(*args, **kwargs)\n",
    "    c = compiler.Compiler(model)\n",
    "    return c.mdp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e759ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "0.05\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.35\n",
      "0.4\n",
      "0.45\n",
      "0.49\n"
     ]
    }
   ],
   "source": [
    "alpha = [0.01, 0.05, 0.1, 0.2, 0.3, 0.35, 0.4, 0.45, 0.49]\n",
    "ptmdp = []\n",
    "mdp = []\n",
    "for a in alpha:\n",
    "    print(a)\n",
    "    m = bitcoin_mdp(alpha=a, gamma=0.0, maximum_fork_length=25)\n",
    "    mdp.append(m)\n",
    "    ptmdp.append(barzur20aft.ptmdp(m, horizon=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e448904a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha=0.01 iter= 30 v[0]=0.14 p=[0 2 1 0 0 0 1 0 0 2] 1370 501\n",
      "alpha=0.05 iter=315 v[0]=3.97 p=[0 2 1 0 0 0 1 0 0 2] 1386 1966\n",
      "alpha=0.10 iter=439 v[0]=8.89 p=[0 2 1 0 0 0 1 0 0 2] 1380 582\n",
      "alpha=0.20 iter=535 v[0]=18.64 p=[0 2 1 0 0 0 1 0 0 2] 1350 1185\n",
      "alpha=0.30 iter=541 v[0]=28.02 p=[0 2 1 0 0 0 1 0 0 2] 1302 4164\n",
      "alpha=0.35 iter=571 v[0]=35.46 p=[0 0 1 0 0 0 1 0 0 2] 1282 5217\n",
      "alpha=0.40 iter=628 v[0]=47.30 p=[0 0 1 0 0 0 1 0 0 2] 1260 7227\n",
      "alpha=0.45 iter=689 v[0]=64.32 p=[0 0 1 0 0 0 1 0 0 0] 1236 5698\n",
      "alpha=0.49 iter=747 v[0]=80.97 p=[0 0 1 0 0 0 1 0 0 0] 1200 6423\n"
     ]
    }
   ],
   "source": [
    "vi = []\n",
    "for i, m in enumerate(ptmdp):\n",
    "    res = m.value_iteration(value_eps=0.01)\n",
    "    vi.append(res)\n",
    "    v = res[\"value\"]\n",
    "    p = res[\"policy\"]\n",
    "    iter = res[\"iter\"]\n",
    "    print(\n",
    "        f\"alpha={alpha[i]:.2f} iter={iter:3d} v[0]={v[0]:.2f} p={p[:10]} {sum(p)} {hash(p.tobytes()) % 10000}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f42896f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha=0.01 rpp[0]=0.009999999999999992\n",
      "alpha=0.05 rpp[0]=0.049999999999999954\n",
      "alpha=0.1 rpp[0]=0.1\n",
      "alpha=0.2 rpp[0]=0.19999999999999984\n",
      "alpha=0.3 rpp[0]=0.2999999999999994\n",
      "alpha=0.35 rpp[0]=0.3681152968623869\n",
      "alpha=0.4 rpp[0]=0.48155899463631074\n",
      "alpha=0.45 rpp[0]=0.6372053002323459\n",
      "alpha=0.49 rpp[0]=0.7969679577222449\n"
     ]
    }
   ],
   "source": [
    "def reward_per_progress_backpropagation(mdp, policy, n_iter=500):\n",
    "    reward = np.zeros(mdp.n_states, dtype=float)\n",
    "    progress = np.zeros(mdp.n_states, dtype=float)\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        reward_next = np.zeros(mdp.n_states, dtype=float)\n",
    "        progress_next = np.zeros(mdp.n_states, dtype=float)\n",
    "        for src in range(mdp.n_states):\n",
    "            act = policy[src]\n",
    "            if act == -1:\n",
    "                assert len(mdp.tab[src]) == 0\n",
    "                continue\n",
    "            for t in mdp.tab[src][act]:\n",
    "                reward_next[src] += t.probability * (t.reward + reward[t.destination])\n",
    "                progress_next[src] += t.probability * (\n",
    "                    t.progress + progress[t.destination]\n",
    "                )\n",
    "        reward = reward_next\n",
    "        progress = progress_next\n",
    "    return reward / progress\n",
    "\n",
    "\n",
    "for i, a in enumerate(alpha):\n",
    "    rpp = reward_per_progress_backpropagation(mdp[i], vi[i][\"policy\"])\n",
    "    print(f\"alpha={a} rpp[0]={rpp[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cff1c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55651/849863093.py:33: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return np.sum(np.multiply(vec, rew)) / np.sum(np.multiply(vec, prg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha=0.01 rpp=nan\n",
      "alpha=0.05 rpp=nan\n",
      "alpha=0.1 rpp=nan\n",
      "alpha=0.2 rpp=nan\n",
      "alpha=0.3 rpp=nan\n",
      "alpha=0.35 rpp=0.37061541132797327\n",
      "alpha=0.4 rpp=0.48705470407321894\n",
      "alpha=0.45 rpp=0.6527894974063071\n",
      "alpha=0.49 rpp=0.8283342996887935\n"
     ]
    }
   ],
   "source": [
    "def steady_state_rpp(mdp, policy):\n",
    "    n = mdp.n_states\n",
    "    prb = np.zeros((n, n), dtype=float)\n",
    "    rew = np.zeros(n, dtype=float)\n",
    "    prg = np.zeros(n, dtype=float)\n",
    "\n",
    "    for src, actions in enumerate(mdp.tab):\n",
    "        for t in actions[policy[src]]:\n",
    "            dst = t.destination\n",
    "            prb[src, dst] = t.probability\n",
    "            rew[src] += t.probability * t.reward\n",
    "            prg[src] += t.probability * t.progress\n",
    "\n",
    "    # by squaring the matrix we can do 2^10 state transitions quickly\n",
    "    for _ in range(10):\n",
    "        prb = np.dot(prb, prb)\n",
    "\n",
    "    vec = np.zeros(n, dtype=float)\n",
    "    for s, p in mdp.start.items():\n",
    "        vec[s] = p\n",
    "\n",
    "    assert sum(vec) == 1, f\"{sum(vec)}\"\n",
    "    # print('start states', sum(vec > 0))\n",
    "\n",
    "    vec = np.dot(vec, prb)\n",
    "    assert math.isclose(sum(vec), 1), f\"{sum(vec)}\"\n",
    "    # print('steady states', sum(vec > 0))\n",
    "\n",
    "    # print(vec.shape, rew.shape, prg.shape)\n",
    "    # print(np.sum(np.multiply(vec, rew)))\n",
    "    # print(np.sum(np.multiply(vec, prg)))\n",
    "\n",
    "    return np.sum(np.multiply(vec, rew)) / np.sum(np.multiply(vec, prg))\n",
    "\n",
    "\n",
    "for i, a in enumerate(alpha):\n",
    "    rpp = steady_state_rpp(mdp[i], vi[i][\"policy\"])\n",
    "    print(f\"alpha={a} rpp={rpp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c08fd3",
   "metadata": {},
   "source": [
    "**Not sure why this does not calculate the steady state rewards for the honest policy?!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7fc4758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha=0.01 revenue=0.009999999999999992\n",
      "alpha=0.05 revenue=0.04999999999999995\n",
      "alpha=0.1 revenue=0.1\n",
      "alpha=0.2 revenue=0.20000000000000698\n",
      "alpha=0.3 revenue=0.2999999999999994\n",
      "alpha=0.35 revenue=0.370568830736789\n",
      "alpha=0.4 revenue=0.48691846604104705\n",
      "alpha=0.45 revenue=0.6524747897127535\n",
      "alpha=0.49 revenue=0.8280881459620522\n"
     ]
    }
   ],
   "source": [
    "def steady_state(mdp, policy):\n",
    "    n = mdp.n_states\n",
    "    prb = np.zeros((n, n), dtype=float)\n",
    "    rew = np.zeros(n, dtype=float)\n",
    "    prg = np.zeros(n, dtype=float)\n",
    "\n",
    "    for src, actions in enumerate(mdp.tab):\n",
    "        for t in actions[policy[src]]:\n",
    "            dst = t.destination\n",
    "            prb[src, dst] = t.probability\n",
    "            rew[src] += t.probability * t.reward\n",
    "            prg[src] += t.probability * t.progress\n",
    "\n",
    "    # by squaring the matrix we can do 2^10 state transitions quickly\n",
    "    for _ in range(10):\n",
    "        prb = np.dot(prb, prb)\n",
    "\n",
    "    vec = np.zeros(n, dtype=float)\n",
    "    for s, p in mdp.start.items():\n",
    "        vec[s] = p\n",
    "\n",
    "    assert sum(vec) == 1, f\"{sum(vec)}\"\n",
    "\n",
    "    vec = np.dot(vec, prb)\n",
    "    assert math.isclose(sum(vec), 1), f\"{sum(vec)}\"\n",
    "\n",
    "    return vec\n",
    "\n",
    "\n",
    "for i, a in enumerate(alpha):\n",
    "    ss = steady_state(mdp[i], vi[i][\"policy\"])\n",
    "    rpp = reward_per_progress_backpropagation(mdp[i], vi[i][\"policy\"])\n",
    "    revenue = sum(np.multiply(ss, rpp))\n",
    "    print(f\"alpha={a} revenue={revenue}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
