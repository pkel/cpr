{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "024cda7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import barzur20aft\n",
    "import compiler\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ab5a2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitcoin_mdp(*args, **kwargs):\n",
    "    model = barzur20aft.Bitcoin(*args, **kwargs)\n",
    "    c = compiler.Compiler(model)\n",
    "    return c.mdp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e759ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "0.05\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.35\n",
      "0.4\n",
      "0.45\n",
      "0.49\n"
     ]
    }
   ],
   "source": [
    "alpha = [0.01, 0.05, 0.1, 0.2, 0.3, 0.35, 0.4, 0.45, 0.49]\n",
    "ptmdp = []\n",
    "mdp = []\n",
    "for a in alpha:\n",
    "    print(a)\n",
    "    m = bitcoin_mdp(alpha=a, gamma=1, maximum_fork_length=25)\n",
    "    mdp.append(m)\n",
    "    ptmdp.append(barzur20aft.ptmdp(m, horizon=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e448904a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha=0.01 iter= 30 v[0]=0.14 p=[0 0 1 0 3 0 1 0 0 0] 1046 5541\n",
      "alpha=0.05 iter=195 v[0]=3.23 p=[0 0 1 0 3 0 1 0 0 0] 949 1250\n",
      "alpha=0.10 iter=351 v[0]=9.04 p=[0 0 1 0 3 0 1 0 0 0] 861 8136\n",
      "alpha=0.20 iter=534 v[0]=22.87 p=[0 0 1 0 3 0 1 0 0 0] 771 8455\n",
      "alpha=0.30 iter=664 v[0]=40.86 p=[0 0 1 0 3 0 1 0 0 0] 782 5254\n",
      "alpha=0.35 iter=718 v[0]=52.17 p=[0 0 1 0 3 0 1 0 0 0] 773 4366\n",
      "alpha=0.40 iter=763 v[0]=65.48 p=[0 0 1 0 3 0 1 0 0 0] 778 7242\n",
      "alpha=0.45 iter=795 v[0]=80.41 p=[0 0 1 0 3 0 1 0 0 0] 733 1726\n",
      "alpha=0.49 iter=804 v[0]=91.34 p=[0 0 1 0 3 0 1 0 0 0] 660 97\n"
     ]
    }
   ],
   "source": [
    "vi = []\n",
    "for i, m in enumerate(ptmdp):\n",
    "    res = m.value_iteration(value_eps=0.01)\n",
    "    vi.append(res)\n",
    "    v = res[\"value\"]\n",
    "    p = res[\"policy\"]\n",
    "    iter = res[\"iter\"]\n",
    "    print(\n",
    "        f\"alpha={alpha[i]:.2f} iter={iter:3d} v[0]={v[0]:.2f} p={p[:10]} {sum(p)} {hash(p.tobytes()) % 10000}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7fc4758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def steady_state(mdp, policy):\n",
    "    n = mdp.n_states\n",
    "    row = []\n",
    "    col = []\n",
    "    val = []\n",
    "\n",
    "    # tutorial: https://math.stackexchange.com/a/2452452\n",
    "\n",
    "    for src, actions in enumerate(mdp.tab):\n",
    "        # markov chain itself\n",
    "        for t in actions[policy[src]]:\n",
    "            row.append(src)\n",
    "            col.append(t.destination)\n",
    "            val.append(t.probability)\n",
    "\n",
    "        # -1 on the diagonal\n",
    "        row.append(src)\n",
    "        col.append(src)\n",
    "        val.append(-1)\n",
    "\n",
    "        # all-ones column\n",
    "        row.append(src)\n",
    "        col.append(n)\n",
    "        val.append(1)\n",
    "\n",
    "    Q = scipy.sparse.csr_matrix((val, (row, col)), shape=(n, n + 1))\n",
    "    QTQ = Q.dot(Q.transpose())\n",
    "    bQT = np.ones(n)\n",
    "\n",
    "    v = scipy.sparse.linalg.spsolve(QTQ, bQT)\n",
    "\n",
    "    assert len(v) == n\n",
    "    assert math.isclose(sum(v), 1)\n",
    "\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03dff86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha=0.01 rpp={'rpp_iter': 2, 'rpp': 0.010100010102153764, 'rpp_time': 0.7390792369842529}\n",
      "alpha=0.05 rpp={'rpp_iter': 2, 'rpp': 0.052631568537553644, 'rpp_time': 0.7956194877624512}\n",
      "alpha=0.1 rpp={'rpp_iter': 2, 'rpp': 0.11111110842012437, 'rpp_time': 0.6771755218505859}\n",
      "alpha=0.2 rpp={'rpp_iter': 2, 'rpp': 0.24999998329376974, 'rpp_time': 0.7360646724700928}\n",
      "alpha=0.3 rpp={'rpp_iter': 2, 'rpp': 0.42854368152419214, 'rpp_time': 0.6683721542358398}\n",
      "alpha=0.35 rpp={'rpp_iter': 2, 'rpp': 0.5381460991650204, 'rpp_time': 0.8028817176818848}\n",
      "alpha=0.4 rpp={'rpp_iter': 2, 'rpp': 0.664887995683018, 'rpp_time': 0.6759014129638672}\n",
      "alpha=0.45 rpp={'rpp_iter': 2, 'rpp': 0.8097040208097033, 'rpp_time': 0.8398287296295166}\n",
      "alpha=0.49 rpp={'rpp_iter': 2, 'rpp': 0.9197954947739676, 'rpp_time': 0.7351117134094238}\n"
     ]
    }
   ],
   "source": [
    "def reward_per_progress(mdp, policy, n_iter=0, eps=0, verbose=False):\n",
    "    start = time()\n",
    "\n",
    "    n = mdp.n_states\n",
    "    reward = np.zeros((2, n), dtype=float)\n",
    "    progress = np.zeros((2, n), dtype=float)\n",
    "    prev_rpp = float(\"-inf\")\n",
    "\n",
    "    ss = steady_state(mdp, policy)\n",
    "\n",
    "    i = 1\n",
    "    while True:\n",
    "        prev = i % 2\n",
    "        next = (prev + 1) % 2\n",
    "\n",
    "        for src, actions in enumerate(mdp.tab):\n",
    "            act = policy[src]\n",
    "\n",
    "            if act < 0:\n",
    "                # no action possible in this state\n",
    "                assert len(actions) == 0\n",
    "                continue\n",
    "\n",
    "            for t in actions[act]:\n",
    "                reward[next, src] += t.probability * (\n",
    "                    t.reward + reward[prev, t.destination]\n",
    "                )\n",
    "                progress[next, src] += t.probability * (\n",
    "                    t.progress + progress[prev, t.destination]\n",
    "                )\n",
    "\n",
    "        steady_reward = np.multiply(ss, reward[next,])\n",
    "        steady_progress = np.multiply(ss, progress[next,])\n",
    "        next_rpp = sum(steady_reward) / sum(steady_progress)\n",
    "        delta = np.abs(prev_rpp - next_rpp)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\riteration {i}: rpp={next_rpp} delta={delta}\", end=\"\")\n",
    "\n",
    "        if n_iter > 0 and i >= n_iter:\n",
    "            break\n",
    "        elif eps > 0 and delta < eps:\n",
    "            break\n",
    "        else:\n",
    "            i += 1\n",
    "            prev_rpp = next_rpp\n",
    "\n",
    "    if verbose:\n",
    "        print()  # new line to finish verbose progress bar\n",
    "\n",
    "    return dict(rpp_iter=i, rpp=next_rpp, rpp_time=time() - start)\n",
    "\n",
    "\n",
    "for i, a in enumerate(alpha):\n",
    "    rpp = reward_per_progress(mdp[i], vi[i][\"policy\"], eps=0.0001)\n",
    "    print(f\"alpha={a} rpp={rpp}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
