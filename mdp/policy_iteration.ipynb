{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab192e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from barzur20aft import Bitcoin, map_params, mappable_params, ptmdp\n",
    "from compiler import Compiler\n",
    "from time import time\n",
    "import sm\n",
    "import bitcoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a775a810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MDP of size 1624 / 4 / 6318 / 3.9"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp = Compiler(Bitcoin(**mappable_params, maximum_fork_length=25)).mdp()\n",
    "mdp = map_params(mdp, alpha=0.33, gamma=0.75)\n",
    "mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0efb8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    mdp = Compiler(\n",
    "        sm.SelfishMining(bitcoin.Bitcoin(), **sm.mappable_params, maximum_height=6)\n",
    "    ).mdp()\n",
    "    mdp = sm.map_params(mdp, alpha=0.33, gamma=0.75)\n",
    "    mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85a36253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper: get steady state in MDP space\n",
    "def steady_state(mdp, policy, state):\n",
    "    mc = mdp.markov_chain(policy, start_state=state)\n",
    "    mc_ss = mdp.steady_state(mc[\"prb\"]).pop(\"ss\")\n",
    "\n",
    "    ss = numpy.zeros(policy.shape, dtype=float)\n",
    "    for mc_state, mdp_state in enumerate(mc[\"mdp_states\"]):\n",
    "        ss[mdp_state] = mc_ss[mc_state]\n",
    "\n",
    "    return ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b0a3c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTO revenue (value iteration): 43.99737010919144\n"
     ]
    }
   ],
   "source": [
    "# benchmark: PTO + value iteration\n",
    "pmdp = ptmdp(mdp, horizon=100)\n",
    "vi = pmdp.value_iteration(stop_delta=0.001)\n",
    "\n",
    "vi_ss = steady_state(mdp, vi[\"vi_policy\"], numpy.argmax(\"vi_value\"))\n",
    "\n",
    "vi_revenue = vi[\"vi_value\"].dot(vi_ss)\n",
    "print(\"PTO revenue (value iteration):\", vi_revenue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e728fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTO revenue (value iteration):   43.99737010919144\n",
      "PTO revenue (policy evaluation): 43.99687926382505\n"
     ]
    }
   ],
   "source": [
    "# wip: PTO + policy iteration (policy evaluation only)\n",
    "def policy_evaluation(self, policy, *args, theta, discount=1, verbose=False):\n",
    "    value = numpy.zeros((2, self.n_states), dtype=float)\n",
    "\n",
    "    i = 1\n",
    "    while True:\n",
    "        prev = i % 2\n",
    "        next = (prev + 1) % 2\n",
    "\n",
    "        for src, actions in enumerate(self.tab):\n",
    "            a = policy[src]\n",
    "            if a < 0:\n",
    "                continue\n",
    "            v = 0.0\n",
    "            for t in actions[a]:\n",
    "                v += t.probability * (t.reward + discount * value[prev, t.destination])\n",
    "            value[next, src] = v\n",
    "\n",
    "        delta = numpy.abs(value[next,] - value[prev,]).max()\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\riteration {i}: delta {delta:g}\")\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return value[next,]\n",
    "\n",
    "\n",
    "pe_value = policy_evaluation(pmdp, vi[\"vi_policy\"], theta=0.001)\n",
    "pe_revenue = pe_value.dot(vi_ss)\n",
    "print(\"PTO revenue (value iteration):  \", vi_revenue)\n",
    "print(\"PTO revenue (policy evaluation):\", pe_revenue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "333c550e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTO revenue (value iteration):      43.99737010919144\n",
      "PTO revenue (policy evaluation):    43.99687926382505\n",
      "PTO revenue (policy evaluation ro): 43.99687926382505\n"
     ]
    }
   ],
   "source": [
    "# wip: PTO + policy iteration (policy evaluation only) (reachable only)\n",
    "def policy_evaluation_ro(\n",
    "    self, policy, *args, theta, discount=1, verbose=False, start_state=None\n",
    "):\n",
    "    value = numpy.zeros((2, self.n_states), dtype=float)\n",
    "\n",
    "    reachable = self.reachable_states(policy, start_state=start_state)\n",
    "\n",
    "    i = 1\n",
    "    while True:\n",
    "        prev = i % 2\n",
    "        next = (prev + 1) % 2\n",
    "\n",
    "        for src in reachable:\n",
    "            a = policy[src]\n",
    "            if a < 0:\n",
    "                continue\n",
    "            v = 0.0\n",
    "            for t in self.tab[src][a]:\n",
    "                v += t.probability * (t.reward + discount * value[prev, t.destination])\n",
    "            value[next, src] = v\n",
    "\n",
    "        delta = numpy.abs(value[next,] - value[prev,]).max()\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\riteration {i}: delta {delta:g}\")\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return value[next,]\n",
    "\n",
    "\n",
    "pero_value = policy_evaluation_ro(pmdp, vi[\"vi_policy\"], theta=0.001)\n",
    "pero_revenue = pero_value.dot(vi_ss)\n",
    "print(\"PTO revenue (value iteration):     \", vi_revenue)\n",
    "print(\"PTO revenue (policy evaluation):   \", pe_revenue)\n",
    "print(\"PTO revenue (policy evaluation ro):\", pero_revenue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "471fa46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTO revenue (value iteration):      43.99737010919144\n",
      "PTO revenue (policy evaluation):    43.99687926382505\n",
      "PTO revenue (policy evaluation ro): 43.99687926382505\n",
      "PTO revenue (policy iteration):     43.99687926382505\n",
      "PTO revenue (policy iteration ro):  12.624201898191922\n"
     ]
    }
   ],
   "source": [
    "# wip: PTO + policy iteration\n",
    "def policy_iteration(\n",
    "    self, *args, theta, discount=1, verbose=False, reachable_only=True\n",
    "):\n",
    "    start = time()\n",
    "\n",
    "    policy = numpy.full(self.n_states, -1, dtype=int)\n",
    "\n",
    "    if reachable_only:\n",
    "        best_state = None\n",
    "\n",
    "    i = 1\n",
    "    while True:\n",
    "        stable = True\n",
    "\n",
    "        if reachable_only:\n",
    "            value = policy_evaluation_ro(\n",
    "                self, policy, theta=theta, discount=discount, start_state=best_state\n",
    "            )\n",
    "            best_state = numpy.argmax(value)\n",
    "        else:\n",
    "            value = policy_evaluation(self, policy, theta=theta, discount=discount)\n",
    "\n",
    "        for src, actions in enumerate(self.tab):\n",
    "            best_v = float(\"-inf\")\n",
    "            best_a = -1  # no action possible\n",
    "            for a, lst in actions.items():\n",
    "                if a < 0:\n",
    "                    continue\n",
    "                v = 0.0\n",
    "                for t in lst:\n",
    "                    v += t.probability * (t.reward + discount * value[t.destination])\n",
    "                if v > best_v:\n",
    "                    best_v = v\n",
    "                    best_a = a\n",
    "\n",
    "            if policy[src] != best_a:\n",
    "                stable = False\n",
    "\n",
    "            policy[src] = best_a\n",
    "\n",
    "        if stable:\n",
    "            break\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return dict(pi_value=value, pi_policy=policy, pi_iter=i, pi_time=time() - start)\n",
    "\n",
    "\n",
    "pi = policy_iteration(pmdp, theta=0.001, reachable_only=False)\n",
    "pi_value = pi.pop(\"pi_value\")\n",
    "pi_ss = steady_state(mdp, pi[\"pi_policy\"], numpy.argmax(pi_value))\n",
    "pi_revenue = pi_value.dot(pi_ss)\n",
    "\n",
    "piro = policy_iteration(pmdp, theta=0.001, reachable_only=True)\n",
    "piro_value = piro.pop(\"pi_value\")\n",
    "piro_ss = steady_state(mdp, piro[\"pi_policy\"], numpy.argmax(piro_value))\n",
    "piro_revenue = piro_value.dot(piro_ss)\n",
    "print(\"PTO revenue (value iteration):     \", vi_revenue)\n",
    "print(\"PTO revenue (policy evaluation):   \", pe_revenue)\n",
    "print(\"PTO revenue (policy evaluation ro):\", pero_revenue)\n",
    "print(\"PTO revenue (policy iteration):    \", pi_revenue)\n",
    "print(\"PTO revenue (policy iteration ro): \", piro_revenue)\n",
    "\n",
    "# conclude: reachable-only does work for evaluation but not for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8d1ea45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTO revenue (value iteration):      43.99737010919144 15.032769203186035\n",
      "PTO revenue (policy evaluation):    43.99687926382505\n",
      "PTO revenue (policy evaluation ro): 43.99687926382505\n",
      "PTO revenue (policy iteration):     43.99687926382505 75.58411908149719\n",
      "PTO revenue (policy iteration rvf): 44.01740551368594 20.434382677078247\n"
     ]
    }
   ],
   "source": [
    "# wip: PTO + policy iteration (reuse value function)\n",
    "def policy_evaluation_rvf(\n",
    "    self, policy, *args, theta, discount=1, verbose=False, init=None\n",
    "):\n",
    "    value = numpy.zeros((2, self.n_states), dtype=float)\n",
    "\n",
    "    if init is not None:\n",
    "        value[0,] = init\n",
    "        value[1,] = init\n",
    "\n",
    "    i = 1\n",
    "    while True:\n",
    "        prev = i % 2\n",
    "        next = (prev + 1) % 2\n",
    "\n",
    "        for src, actions in enumerate(self.tab):\n",
    "            a = policy[src]\n",
    "            if a < 0:\n",
    "                continue\n",
    "            v = 0.0\n",
    "            for t in actions[a]:\n",
    "                v += t.probability * (t.reward + discount * value[prev, t.destination])\n",
    "            value[next, src] = v\n",
    "\n",
    "        delta = numpy.abs(value[next,] - value[prev,]).max()\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\riteration {i}: delta {delta:g}\")\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return value[next,]\n",
    "\n",
    "\n",
    "def policy_iteration_rvf(self, *args, theta, discount=1, verbose=False):\n",
    "    start = time()\n",
    "\n",
    "    policy = numpy.full(self.n_states, -1, dtype=int)\n",
    "    value = policy_evaluation_rvf(self, policy, theta=theta, discount=discount)\n",
    "\n",
    "    i = 1\n",
    "    while True:\n",
    "        stable = True\n",
    "\n",
    "        for src, actions in enumerate(self.tab):\n",
    "            best_v = float(\"-inf\")\n",
    "            best_a = -1  # no action possible\n",
    "            for a, lst in actions.items():\n",
    "                if a < 0:\n",
    "                    continue\n",
    "                v = 0.0\n",
    "                for t in lst:\n",
    "                    v += t.probability * (t.reward + discount * value[t.destination])\n",
    "                if v > best_v:\n",
    "                    best_v = v\n",
    "                    best_a = a\n",
    "\n",
    "            if policy[src] != best_a:\n",
    "                stable = False\n",
    "\n",
    "            policy[src] = best_a\n",
    "\n",
    "        if stable:\n",
    "            break\n",
    "\n",
    "        i += 1\n",
    "        value = policy_evaluation_rvf(\n",
    "            self, policy, theta=theta, discount=discount, init=value\n",
    "        )\n",
    "\n",
    "    return dict(pi_value=value, pi_policy=policy, pi_iter=i, pi_time=time() - start)\n",
    "\n",
    "\n",
    "pirvf = policy_iteration_rvf(pmdp, theta=0.001)\n",
    "pirvf_value = pirvf.pop(\"pi_value\")\n",
    "pirvf_ss = steady_state(mdp, pirvf[\"pi_policy\"], numpy.argmax(pirvf_value))\n",
    "pirvf_revenue = pirvf_value.dot(pirvf_ss)\n",
    "print(\"PTO revenue (value iteration):     \", vi_revenue, vi[\"vi_time\"])\n",
    "print(\"PTO revenue (policy evaluation):   \", pe_revenue)\n",
    "print(\"PTO revenue (policy evaluation ro):\", pero_revenue)\n",
    "print(\"PTO revenue (policy iteration):    \", pi_revenue, pi[\"pi_time\"])\n",
    "print(\"PTO revenue (policy iteration rvf):\", pirvf_revenue, pirvf[\"pi_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6780ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RPP (value iteration):  0.30038634434802663\n",
      "RPP (policy iteration): 0.30038634434802663\n"
     ]
    }
   ],
   "source": [
    "def reward_per_progress(mdp, policy, start_state):\n",
    "    mc = mdp.markov_chain(policy, start_state=start_state)\n",
    "    mc.pop(\"mdp_states\")\n",
    "    ss = mdp.steady_state(mc[\"prb\"])\n",
    "    ss_vec = ss.pop(\"ss\")\n",
    "    return mdp.reward_per_progress(policy, **mc, ss=ss_vec, eps=0.001)\n",
    "\n",
    "\n",
    "rpp_vi = reward_per_progress(mdp, vi[\"vi_policy\"], numpy.argmax(vi[\"vi_value\"]))\n",
    "rpp_pi = reward_per_progress(mdp, pirvf[\"pi_policy\"], numpy.argmax(pirvf_value))\n",
    "print(\"RPP (value iteration): \", rpp_vi[\"rpp\"])\n",
    "print(\"RPP (policy iteration):\", rpp_pi[\"rpp\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
