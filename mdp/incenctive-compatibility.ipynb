{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e82b267-af0c-4b9f-9a8b-16c8650b08ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bitcoin import Bitcoin\n",
    "import model\n",
    "import sm\n",
    "import mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfb7b3cb-b009-4eee-81f6-26417a5b000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "protocol = Bitcoin()\n",
    "alpha = 0.25\n",
    "gamma = 0.25\n",
    "\n",
    "# implicit SM model\n",
    "# careful: this truncates the common chain; that mechanism is easy to define for Bitcoin but might cause headaches for other protocols\n",
    "# take in mind that the (absence of) truncation might affect the rest of the algorithm\n",
    "base_model = sm.SelfishMining(protocol, alpha=alpha, gamma=gamma, maximum_size=1000000)\n",
    "\n",
    "terminal_state = b\"\"\n",
    "horizon = 100\n",
    "\n",
    "# implicit SM model with PTO applied on the fly\n",
    "pto_model = model.PTO_wrapper(\n",
    "    base_model, horizon=horizon, terminal_state=terminal_state\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751a8334-06ad-4d84-8ae4-a1d11fe3f9f9",
   "metadata": {},
   "source": [
    "# Level 0: explore honest policy, derive MC, evaluate\n",
    "\n",
    "#### L0.1: explore states and build MC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e909d654-d09a-4afa-820b-2dd67f3b91e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(0, 0.25), (2, 0.75)],\n",
       " [[(1, 0.0625, 0.0, 0),\n",
       "   (7, 0.1875, 0.0, 0),\n",
       "   (1, 0.1875, 0.0, 0),\n",
       "   (7, 0.5625, 0.0, 0)],\n",
       "  [(5, 1, 0.0, 0)],\n",
       "  [(3, 0.0625, 0.0, 0),\n",
       "   (6, 0.1875, 0.0, 0),\n",
       "   (3, 0.1875, 0.0, 0),\n",
       "   (6, 0.5625, 0.0, 0)],\n",
       "  [(4, 1, 0.0, 0)],\n",
       "  [(1, 0.0625, 1.0, 1),\n",
       "   (7, 0.1875, 1.0, 1),\n",
       "   (1, 0.1875, 1.0, 1),\n",
       "   (7, 0.5625, 1.0, 1)],\n",
       "  [(1, 0.0625, 1.0, 1),\n",
       "   (7, 0.1875, 1.0, 1),\n",
       "   (1, 0.1875, 1.0, 1),\n",
       "   (7, 0.5625, 1.0, 1)],\n",
       "  [(9, 1, 0.0, 0)],\n",
       "  [(8, 1, 0.0, 0)],\n",
       "  [(3, 0.0625, 0.0, 1),\n",
       "   (6, 0.1875, 0.0, 1),\n",
       "   (3, 0.1875, 0.0, 1),\n",
       "   (6, 0.5625, 0.0, 1)],\n",
       "  [(3, 0.0625, 0.0, 1),\n",
       "   (6, 0.1875, 0.0, 1),\n",
       "   (3, 0.1875, 0.0, 1),\n",
       "   (6, 0.5625, 0.0, 1)],\n",
       "  []])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will probably not terminate when we remove truncation!\n",
    "def explore_policy(m, policy):\n",
    "    states = dict()\n",
    "    queue = set()\n",
    "\n",
    "    for s, p in m.start():\n",
    "        queue.add(s)\n",
    "\n",
    "    while len(queue) > 0:\n",
    "        s = queue.pop()\n",
    "\n",
    "        assert s not in states\n",
    "        states[s] = []\n",
    "\n",
    "        a = policy(s)\n",
    "        for t in m.apply(a, s):\n",
    "            if t.probability == 0:\n",
    "                continue\n",
    "\n",
    "            states[s].append((t.state, t.probability, t.reward, t.progress))\n",
    "\n",
    "            if t.state in states or t.state == terminal_state:\n",
    "                continue\n",
    "            else:\n",
    "                queue.add(t.state)\n",
    "\n",
    "    states[terminal_state] = []  # no transitions available in terminal_state\n",
    "\n",
    "    return m.start(), states\n",
    "\n",
    "\n",
    "def explore_honest(m):\n",
    "    return explore_policy(m, m.honest)\n",
    "\n",
    "\n",
    "def with_int_states(mc):\n",
    "    start, tx = mc\n",
    "    table = {s: i for i, s in enumerate(tx.keys())}\n",
    "    istart = []\n",
    "    for s0, p in start:\n",
    "        istart.append((table[s0], p))\n",
    "    istart = sorted(istart)\n",
    "    itx = []\n",
    "    for s0, tx_lst in tx.items():\n",
    "        itx.append([(table[s1], prb, rew, prg) for s1, prb, rew, prg in tx_lst])\n",
    "    return istart, itx\n",
    "\n",
    "\n",
    "mc = explore_honest(base_model)\n",
    "with_int_states(mc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216bdecf-3c58-44a4-a08a-3c3a290c6864",
   "metadata": {},
   "source": [
    "#### L0.2: evaluate MC; RTDP style\n",
    "\n",
    "- random walk through the MC\n",
    "- back-propagate values\n",
    "\n",
    "This should converge quickly initially as is focuses on important states.\n",
    "For the same reason it will slow down when getting closer to the true limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbc97e4a-2ab3-4beb-bdf9-1c4223131b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def sample_next_state_and_map(tx_lst, fun):\n",
    "    x = random.random()\n",
    "    i = None\n",
    "    acc_prb = 0.0\n",
    "    ret = []\n",
    "    for j, tx in enumerate(tx_lst):\n",
    "        ret.append(fun(tx))\n",
    "        if acc_prb <= x:\n",
    "            i = j\n",
    "        acc_prb += tx[1]\n",
    "    next_state = tx_lst[i][0]\n",
    "    return next_state, ret\n",
    "\n",
    "\n",
    "def mc_path_backpropagation_eval(m, policy):\n",
    "    start, tx = with_int_states(explore_policy(m, policy))\n",
    "\n",
    "    count = [0] * len(tx)\n",
    "    value = [0.0] * len(tx)\n",
    "    prg_v = [0.0] * len(tx)\n",
    "    state, _ = sample_next_state_and_map(start, lambda x: None)\n",
    "    terminating = False\n",
    "\n",
    "    n_steps = 100000\n",
    "    for i in range(n_steps):\n",
    "        if len(tx[state]) < 1:  # terminal state\n",
    "            terminating = True\n",
    "            state, _ = sample_next_state_and_map(start, lambda x: None)\n",
    "\n",
    "        def fun(t):\n",
    "            next_state, prb, rew, prg = t\n",
    "            return prb * (rew + value[next_state]), prb * (prg + prg_v[next_state])\n",
    "\n",
    "        next_state, values_and_prg_vs = sample_next_state_and_map(tx[state], fun)\n",
    "        values, prg_vs = zip(*values_and_prg_vs)\n",
    "        value[state] = sum(values)\n",
    "        prg_v[state] = sum(prg_vs)\n",
    "        count[next_state] += 1\n",
    "        state = next_state\n",
    "\n",
    "    # start state estimate\n",
    "    start_value_estimate = sum([value[s] * p for s, p in start])\n",
    "    start_prg_v_estimate = sum([prg_v[s] * p for s, p in start])\n",
    "\n",
    "    if terminating:\n",
    "        return start_value_estimate, start_prg_v_estimate, value, prg_v\n",
    "\n",
    "    else:\n",
    "        # steady state estimate\n",
    "        # note: steady state don't make sense for PTO; the terminal state is the steady state!\n",
    "        assert sum(count) == n_steps\n",
    "        steady_state = [c / n_steps for c in count]\n",
    "        steady_state_value = sum([v * c for v, c in zip(value, count)]) / n_steps\n",
    "        steady_state_prg_v = sum([v * c for v, c in zip(prg_v, count)]) / n_steps\n",
    "\n",
    "        return (\n",
    "            start_value_estimate,\n",
    "            start_prg_v_estimate,\n",
    "            value,\n",
    "            prg_v,\n",
    "            steady_state,\n",
    "            steady_state_value,\n",
    "            steady_state_prg_v,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "485c28b6-164a-4e8b-9576-12be25f1f81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.24999999999999736, 0.25)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_reward, expected_progress, _, _ = mc_path_backpropagation_eval(\n",
    "    pto_model, pto_model.honest\n",
    ")\n",
    "assert abs(expected_progress - horizon) < 0.00001\n",
    "expected_reward / expected_progress, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9d08a1e-ad16-4918-b4b3-1d12a03cc2dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.24999189198911073, 0.25)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, _, _, _, _, expected_reward, expected_progress = mc_path_backpropagation_eval(\n",
    "    base_model, base_model.honest\n",
    ")\n",
    "expected_reward / expected_progress, alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec5f608-03f8-404b-9010-8ce98258bc5d",
   "metadata": {},
   "source": [
    "# Level 1: explore MDP around honest policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3af3fc3c-a632-4e4b-bf52-2d3861d837e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "258"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will probably not terminate when we remove common chain truncation in the selfish mining model!\n",
    "\n",
    "\n",
    "def explore_around_policy(m, policy, max_distance=0):\n",
    "    states = (\n",
    "        dict()\n",
    "    )  # state -> action -> list of transitions (next state, probability, reward, progress, ...)\n",
    "    queue = set()  # states to be explored\n",
    "    distances = dict()  # state -> distance to honest policy\n",
    "\n",
    "    def distance(s):\n",
    "        return distances.get(s, float(\"inf\"))\n",
    "\n",
    "    for s, p in m.start():\n",
    "        queue.add(s)\n",
    "        distances[s] = 0\n",
    "\n",
    "    while len(queue) > 0:\n",
    "        s = queue.pop()\n",
    "        d = distances[s]\n",
    "\n",
    "        assert s not in states\n",
    "        states[s] = dict()\n",
    "\n",
    "        h = policy(s)\n",
    "        for a in m.actions(s):\n",
    "            states[s][a] = []\n",
    "            for t in m.apply(a, s):\n",
    "                if t.probability == 0:\n",
    "                    continue\n",
    "\n",
    "                states[s][a].append(t)\n",
    "\n",
    "                if a == h:\n",
    "                    distances[t.state] = min(distance(t.state), distances[s])\n",
    "                else:\n",
    "                    distances[t.state] = min(distance(t.state), distances[s] + 1)\n",
    "                # I'm not sure about this mechanism\n",
    "                # What if we find a new path to a state with smaller distance than known before?\n",
    "                # What's the distance of state that can be reached with honest policy but only from a state with disctance > 0?\n",
    "                # I guess the right approach is incremental exploration: explore distance = 0; then = 1, and so on.\n",
    "                #\n",
    "                # Edit: This approach is certainly wrong. The resulting mdp size is not deterministic! It's fixed in the\n",
    "                # Explorer class below.\n",
    "\n",
    "                if distances[t.state] > max_distance:\n",
    "                    continue\n",
    "\n",
    "                if t.state in states or t.state == terminal_state:\n",
    "                    continue\n",
    "                else:\n",
    "                    queue.add(t.state)\n",
    "\n",
    "    states[terminal_state] = []  # no transitions available in terminal_state\n",
    "\n",
    "    return m.start(), states\n",
    "\n",
    "\n",
    "start, m = explore_around_policy(pto_model, pto_model.honest, max_distance=3)\n",
    "len(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "940a93db-e929-4a9e-9889-6756df466126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 11\n",
      "1 37 3.3636363636363638\n",
      "2 111 3.0\n",
      "3 279 2.5135135135135136\n",
      "4 601 2.154121863799283\n",
      "5 1166 1.940099833610649\n"
     ]
    }
   ],
   "source": [
    "# implementing the above loop as incremental exploration\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "# Invariant:\n",
    "# - policy-defined actions are explored first\n",
    "# - in the result MDP: policy = lambda s: 0\n",
    "# - states are enumerated in order of exploration\n",
    "# - states far off the policy have higher state ids than state closer or on the policy\n",
    "# - policies for smaller MDPs are compatible with policies for bigger MDPs\n",
    "class Explorer:\n",
    "    def __init__(self, model, policy):\n",
    "        self.model = model\n",
    "        self.policy = policy\n",
    "\n",
    "        self._mdp = mdp.MDP()\n",
    "        self.states = []  # state id int -> state\n",
    "        self.policy_tab = []  # state id int -> action\n",
    "\n",
    "        self._state_id = dict()  # state -> state id int; see self.state_id()\n",
    "        self.explored_upto = -1  # state id int; policy action has been explored\n",
    "        self.fully_explored_upto = -1  # state id int; all actions have been explored\n",
    "\n",
    "        for s, p in self.model.start():\n",
    "            self._mdp.start[self.state_id(s)] = p\n",
    "\n",
    "    def state_id(self, state):\n",
    "        if state in self._state_id:\n",
    "            return self._state_id[state]\n",
    "        else:\n",
    "            i = len(self._state_id)\n",
    "            self._state_id[state] = i\n",
    "            self.states.append(state)\n",
    "            return i\n",
    "\n",
    "    @property\n",
    "    def n_states(self):\n",
    "        return len(self._state_id)\n",
    "\n",
    "    @property\n",
    "    def max_state_id(self):\n",
    "        return len(self._state_id) - 1\n",
    "\n",
    "    def explore_along_policy(self):\n",
    "        while self.max_state_id > self.explored_upto:\n",
    "            self.explored_upto += 1\n",
    "            s_id = self.explored_upto\n",
    "            s = self.states[s_id]\n",
    "\n",
    "            assert (\n",
    "                len(self.policy_tab) == s_id\n",
    "            ), f\"logic error, {len(self.policy_tab)} == {s_id}\"\n",
    "\n",
    "            if len(self.model.actions(s)) == 0:\n",
    "                # s is a terminal state\n",
    "                self.policy_tab.append(-1)  # policy is not defined, put placeholder\n",
    "                continue\n",
    "\n",
    "            a = self.policy(s)\n",
    "            self.policy_tab.append(a)\n",
    "\n",
    "            for t in self.model.apply(a, s):\n",
    "                if t.probability == 0:\n",
    "                    continue\n",
    "\n",
    "                t = mdp.Transition(\n",
    "                    probability=t.probability,\n",
    "                    destination=self.state_id(t.state),\n",
    "                    reward=t.reward,\n",
    "                    progress=t.progress,\n",
    "                    effect=t.effect,\n",
    "                )\n",
    "                self._mdp.add_transition(src=s_id, act=0, t=t)\n",
    "\n",
    "            self.explored_upto = s_id\n",
    "\n",
    "    def explore_aside_policy(self):\n",
    "        self.explore_along_policy()\n",
    "        assert self.explored_upto == self.max_state_id\n",
    "\n",
    "        while self.fully_explored_upto < self.explored_upto:\n",
    "            self.fully_explored_upto += 1\n",
    "            s_id = self.fully_explored_upto\n",
    "            s = self.states[s_id]\n",
    "\n",
    "            a_idx = 0  # policy action has a_idx = 0\n",
    "            for a in self.model.actions(s):\n",
    "                if a == self.policy_tab[s_id]:\n",
    "                    # policy action; already explored\n",
    "                    continue\n",
    "\n",
    "                a_idx += 1\n",
    "\n",
    "                for t in self.model.apply(a, s):\n",
    "                    if t.probability == 0:\n",
    "                        continue\n",
    "\n",
    "                    t = mdp.Transition(\n",
    "                        probability=t.probability,\n",
    "                        destination=self.state_id(t.state),\n",
    "                        reward=t.reward,\n",
    "                        progress=t.progress,\n",
    "                        effect=t.effect,\n",
    "                    )\n",
    "                    self._mdp.add_transition(src=s_id, act=a_idx, t=t)\n",
    "\n",
    "    def mdp(self):\n",
    "        # Note 1. For some states in self.partial_mdp; we've only explored the honest action.\n",
    "        # That's okay, it forces the attacker to abort the attack.\n",
    "        # Note 2. Some states are reachable but not yet explored; we have to fix this before\n",
    "        # returning the MDP.\n",
    "        self.explore_along_policy()\n",
    "        assert self.explored_upto == self.max_state_id\n",
    "\n",
    "        self._mdp.check()\n",
    "\n",
    "        return deepcopy(self._mdp)\n",
    "\n",
    "\n",
    "e = Explorer(pto_model, pto_model.honest)\n",
    "d = 0\n",
    "m = e.mdp()\n",
    "ms = [m]\n",
    "n = m.n_states\n",
    "print(d, n)\n",
    "for i in range(5):\n",
    "    e.explore_aside_policy()\n",
    "    d += 1\n",
    "    m = e.mdp()\n",
    "    ms.append(m)\n",
    "    prev_n = n\n",
    "    n = m.n_states\n",
    "    print(d, n, n / prev_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770ccec8-4a72-4fc1-9042-53383efcd924",
   "metadata": {},
   "source": [
    "# Level 2: optimize the MDPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c91575-915a-48e0-b3ee-8f3d43f4c644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(m, m_honest):\n",
    "    res = m.value_iteration(stop_delta=0.00001)\n",
    "    res[\"episode_reward\"] = sum(res[\"vi_value\"][s] * p for s, p in m.start.items())\n",
    "    res[\"episode_progress\"] = sum(res[\"vi_progress\"][s] * p for s, p in m.start.items())\n",
    "    res[\"reward_per_progress\"] = res[\"episode_reward\"] / res[\"episode_progress\"]\n",
    "\n",
    "    res[\"fully_honest\"] = all(a < 1 for a in res[\"vi_policy\"])\n",
    "    # It at first came unexpeted to me, that vi_policy has dishonest entries while the honest policy was optimal.\n",
    "    # Turns out the dishonest behavior happens on states that cannot be reached from the honest policy.\n",
    "    # I'm now comparing the policies for only these states which can be rewach from the honest policy.\n",
    "    # If the agent acts honest on these states it acts honest everywhere.\n",
    "    res[\"honest\"] = all(a < 1 for a in res[\"vi_policy\"][: m_honest.n_states])\n",
    "    return res\n",
    "\n",
    "\n",
    "for d, m in enumerate(ms):\n",
    "    res = evaluate(m, ms[0])\n",
    "    print(\n",
    "        d,\n",
    "        m.n_states,\n",
    "        res[\"fully_honest\"],\n",
    "        res[\"honest\"],\n",
    "        res[\"episode_progress\"],\n",
    "        res[\"reward_per_progress\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f31b1d-b9df-4712-bc8f-b18d5f4bb6a4",
   "metadata": {},
   "source": [
    "# Level 3: Find break-even points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f369685-712e-455d-953f-416b477bfd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations = dict()\n",
    "break_even = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d88b11f1-f266-4907-8eeb-33b24d81eba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma = 0, distance = 1\n",
      "no attack for alpha=0.5\n",
      "   attack for alpha=0.75 w/ rpp=0.8538412396784669\n",
      "   attack for alpha=0.625 w/ rpp=0.692499622602182\n",
      "   attack for alpha=0.5625 w/ rpp=0.5982537254499815\n",
      "   attack for alpha=0.53125 w/ rpp=0.5492005527112394\n",
      "   attack for alpha=0.515625 w/ rpp=0.524397590249471\n",
      "   attack for alpha=0.5078125 w/ rpp=0.5119540841505253\n",
      "   attack for alpha=0.50390625 w/ rpp=0.5057254891452536\n",
      "   attack for alpha=0.501953125 w/ rpp=0.502610304950212\n",
      "   attack for alpha=0.5009765625 w/ rpp=0.5010533669691105\n",
      "   attack for alpha=0.50048828125 w/ rpp=0.5002766835536994\n",
      "   attack for alpha=0.500244140625 w/ rpp=0.4998920505373576\n",
      "gamma = 0, distance = 2\n",
      "   attack for alpha=0.5 w/ rpp=0.5526505013144368\n",
      "no attack for alpha=0.25\n",
      "no attack for alpha=0.375\n",
      "   attack for alpha=0.4375 w/ rpp=0.45793663925195627\n",
      "   attack for alpha=0.40625 w/ rpp=0.4100193429350442\n",
      "no attack for alpha=0.390625\n",
      "   attack for alpha=0.3984375 w/ rpp=0.3980694910329672\n",
      "no attack for alpha=0.39453125\n",
      "no attack for alpha=0.396484375\n",
      "no attack for alpha=0.3974609375\n",
      "   attack for alpha=0.39794921875 w/ rpp=0.3973398426470536\n",
      "no attack for alpha=0.397705078125\n",
      "gamma = 0.25, distance = 1\n",
      "   attack for alpha=0.5 w/ rpp=0.5211132725296131\n",
      "no attack for alpha=0.25\n",
      "no attack for alpha=0.375\n",
      "   attack for alpha=0.4375 w/ rpp=0.43970931538809094\n",
      "no attack for alpha=0.40625\n",
      "no attack for alpha=0.421875\n",
      "   attack for alpha=0.4296875 w/ rpp=0.42946098256062964\n",
      "no attack for alpha=0.42578125\n",
      "no attack for alpha=0.427734375\n",
      "   attack for alpha=0.4287109375 w/ rpp=0.4281994072665316\n",
      "no attack for alpha=0.42822265625\n",
      "no attack for alpha=0.428466796875\n",
      "gamma = 0.25, distance = 2\n",
      "   attack for alpha=0.5 w/ rpp=0.5700635540998571\n",
      "no attack for alpha=0.25\n",
      "   attack for alpha=0.375 w/ rpp=0.3854193597045398\n",
      "no attack for alpha=0.3125\n",
      "no attack for alpha=0.34375\n",
      "   attack for alpha=0.359375 w/ rpp=0.3623119857076834\n",
      "no attack for alpha=0.3515625\n",
      "   attack for alpha=0.35546875 w/ rpp=0.35656345857250676\n",
      "   attack for alpha=0.353515625 w/ rpp=0.3536954492122641\n",
      "   attack for alpha=0.3525390625 w/ rpp=0.35226787075609545\n",
      "   attack for alpha=0.35205078125 w/ rpp=0.351789773307001\n",
      "no attack for alpha=0.351806640625\n",
      "gamma = 0.5, distance = 1\n",
      "   attack for alpha=0.5 w/ rpp=0.5450400831492349\n",
      "no attack for alpha=0.25\n",
      "   attack for alpha=0.375 w/ rpp=0.38630564256217226\n",
      "no attack for alpha=0.3125\n",
      "   attack for alpha=0.34375 w/ rpp=0.34629926371470127\n",
      "no attack for alpha=0.328125\n",
      "   attack for alpha=0.3359375 w/ rpp=0.3363425386859376\n",
      "no attack for alpha=0.33203125\n",
      "   attack for alpha=0.333984375 w/ rpp=0.3338594010337929\n",
      "no attack for alpha=0.3330078125\n",
      "   attack for alpha=0.33349609375 w/ rpp=0.33324740755432775\n",
      "no attack for alpha=0.333251953125\n",
      "gamma = 0.5, distance = 2\n",
      "   attack for alpha=0.5 w/ rpp=0.589543199050206\n",
      "no attack for alpha=0.25\n",
      "   attack for alpha=0.375 w/ rpp=0.41158513655630513\n",
      "   attack for alpha=0.3125 w/ rpp=0.32378198689027504\n",
      "   attack for alpha=0.28125 w/ rpp=0.28117883225766716\n",
      "no attack for alpha=0.265625\n",
      "no attack for alpha=0.2734375\n",
      "no attack for alpha=0.27734375\n",
      "no attack for alpha=0.279296875\n",
      "no attack for alpha=0.2802734375\n",
      "   attack for alpha=0.28076171875 w/ rpp=0.28053825549215855\n",
      "no attack for alpha=0.280517578125\n",
      "gamma = 0.75, distance = 1\n",
      "   attack for alpha=0.5 w/ rpp=0.5712239417031103\n",
      "   attack for alpha=0.25 w/ rpp=0.26046093590977787\n",
      "no attack for alpha=0.125\n",
      "no attack for alpha=0.1875\n",
      "   attack for alpha=0.21875 w/ rpp=0.2223371300665868\n",
      "   attack for alpha=0.203125 w/ rpp=0.20363246755552492\n",
      "no attack for alpha=0.1953125\n",
      "no attack for alpha=0.19921875\n",
      "   attack for alpha=0.201171875 w/ rpp=0.2013140123671669\n",
      "   attack for alpha=0.2001953125 w/ rpp=0.20015883793178027\n",
      "no attack for alpha=0.19970703125\n",
      "no attack for alpha=0.199951171875\n",
      "gamma = 0.75, distance = 2\n",
      "   attack for alpha=0.5 w/ rpp=0.6210439575186558\n",
      "   attack for alpha=0.25 w/ rpp=0.27073660349258943\n",
      "no attack for alpha=0.125\n",
      "   attack for alpha=0.1875 w/ rpp=0.19018840215177313\n",
      "no attack for alpha=0.15625\n",
      "no attack for alpha=0.171875\n",
      "   attack for alpha=0.1796875 w/ rpp=0.18057004289578238\n",
      "   attack for alpha=0.17578125 w/ rpp=0.17580584114094716\n",
      "no attack for alpha=0.173828125\n",
      "no attack for alpha=0.1748046875\n",
      "no attack for alpha=0.17529296875\n",
      "   attack for alpha=0.175537109375 w/ rpp=0.17551097818318262\n",
      "gamma = 1, distance = 1\n",
      "   attack for alpha=0.5 w/ rpp=0.5999999867377265\n",
      "   attack for alpha=0.25 w/ rpp=0.2941176329564463\n",
      "   attack for alpha=0.125 w/ rpp=0.13846152897312103\n",
      "   attack for alpha=0.0625 w/ rpp=0.06614785444960722\n",
      "   attack for alpha=0.03125 w/ rpp=0.03219511901783638\n",
      "   attack for alpha=0.015625 w/ rpp=0.015865265768462745\n",
      "   attack for alpha=0.0078125 w/ rpp=0.00787305385376942\n",
      "   attack for alpha=0.00390625 w/ rpp=0.003921448567414789\n",
      "   attack for alpha=0.001953125 w/ rpp=0.0019569320369262604\n",
      "   attack for alpha=0.0009765625 w/ rpp=0.000977515144321229\n",
      "   attack for alpha=0.00048828125 w/ rpp=0.0004885195027882067\n",
      "   attack for alpha=0.000244140625 w/ rpp=0.0002442001896575776\n",
      "gamma = 1, distance = 2\n",
      "   attack for alpha=0.5 w/ rpp=0.6655628493883045\n",
      "   attack for alpha=0.25 w/ rpp=0.3075152910103688\n",
      "   attack for alpha=0.125 w/ rpp=0.1403293381007932\n",
      "   attack for alpha=0.0625 w/ rpp=0.06638744839140584\n",
      "   attack for alpha=0.03125 w/ rpp=0.032225258646744234\n",
      "   attack for alpha=0.015625 w/ rpp=0.015869038297416756\n",
      "   attack for alpha=0.0078125 w/ rpp=0.007873525569365138\n",
      "   attack for alpha=0.00390625 w/ rpp=0.003921507420060585\n",
      "   attack for alpha=0.001953125 w/ rpp=0.001956939305773266\n",
      "   attack for alpha=0.0009765625 w/ rpp=0.0009775160342007957\n",
      "   attack for alpha=0.00048828125 w/ rpp=0.0004885196101010858\n",
      "   attack for alpha=0.000244140625 w/ rpp=0.00024420020255790315\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.0\n",
    "distance = 5\n",
    "precision = 12\n",
    "\n",
    "\n",
    "def localize_be(gamma, distance, precision):\n",
    "    upper_alpha = 1\n",
    "    lower_alpha = 0\n",
    "    for i in range(precision):\n",
    "        alpha = (upper_alpha + lower_alpha) / 2\n",
    "        base_model = sm.SelfishMining(\n",
    "            protocol, alpha=alpha, gamma=gamma, maximum_size=1000000\n",
    "        )\n",
    "        pto_model = model.PTO_wrapper(\n",
    "            base_model, horizon=horizon, terminal_state=terminal_state\n",
    "        )\n",
    "\n",
    "        attack_found = False\n",
    "        e = Explorer(pto_model, pto_model.honest)\n",
    "        m_honest = e.mdp()\n",
    "        for d in range(1, distance + 1):\n",
    "            e.explore_aside_policy()  # increase distance by one\n",
    "            m = e.mdp()\n",
    "\n",
    "        res = evaluate(m, m_honest)\n",
    "        evaluations[(gamma, distance, alpha)] = res\n",
    "        if res[\"honest\"]:\n",
    "            print(f\"no attack for alpha={alpha}\")\n",
    "        else:\n",
    "            rpp = res[\"reward_per_progress\"]\n",
    "            print(f\"   attack for alpha={alpha} w/ rpp={rpp}\")\n",
    "            attack_found = True\n",
    "\n",
    "        if attack_found:\n",
    "            upper_alpha = alpha\n",
    "        else:\n",
    "            lower_alpha = alpha\n",
    "\n",
    "    break_even[(gamma, distance)] = upper_alpha\n",
    "\n",
    "\n",
    "for gamma in [0, 0.25, 0.5, 0.75, 1]:\n",
    "    for distance in range(1, 6):\n",
    "        print(f\"gamma = {gamma}, distance = {distance}\")\n",
    "        localize_be(gamma, distance, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d0b44063-debb-48bc-b1c0-52ea2818b580",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (gamma, distance, alpha), res in evaluations.items():\n",
    "    if (gamma, distance) not in break_even:\n",
    "        break_even[(gamma, distance)] = 1.0\n",
    "\n",
    "    if not res[\"honest\"]:\n",
    "        break_even[(gamma, distance)] = min(break_even[(gamma, distance)], alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2799d9a8-e00d-487c-8008-d7ffd9dd4396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7fe6dbcf6b80>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 569.986x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    [(distance, gamma, be) for (gamma, distance), be in break_even.items()],\n",
    "    columns=[\"distance\", \"gamma\", \"alpha\"],\n",
    ")\n",
    "sns.relplot(data=df, x=\"gamma\", y=\"alpha\", hue=\"distance\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
