{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e82b267-af0c-4b9f-9a8b-16c8650b08ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bitcoin import Bitcoin\n",
    "import model\n",
    "import sm\n",
    "import mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfb7b3cb-b009-4eee-81f6-26417a5b000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "protocol = Bitcoin()\n",
    "alpha = 0.25\n",
    "gamma = 0\n",
    "\n",
    "# implicit SM model\n",
    "# careful: this truncates the common chain; that mechanism is easy to define for Bitcoin but might cause headaches for other protocols\n",
    "# take in mind that the (absence of) truncation might affect the rest of the algorithm\n",
    "base_model = sm.SelfishMining(protocol, alpha=alpha, gamma=gamma, maximum_size=1000000)\n",
    "\n",
    "terminal_state = b\"\"\n",
    "horizon = 100\n",
    "\n",
    "# implicit SM model with PTO applied on the fly\n",
    "pto_model = model.PTO_wrapper(\n",
    "    base_model, horizon=horizon, terminal_state=terminal_state\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751a8334-06ad-4d84-8ae4-a1d11fe3f9f9",
   "metadata": {},
   "source": [
    "# Level 0: explore honest policy, derive MC, evaluate\n",
    "\n",
    "#### L0.1: explore states and build MC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e909d654-d09a-4afa-820b-2dd67f3b91e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(0, 0.75), (1, 0.25)],\n",
       " [[(4, 0.25, 0.0, 0), (5, 0.75, 0.0, 0)],\n",
       "  [(6, 0.25, 0.0, 0), (2, 0.75, 0.0, 0)],\n",
       "  [(3, 1, 0.0, 0)],\n",
       "  [(4, 0.25, 0.0, 1), (5, 0.75, 0.0, 1)],\n",
       "  [(8, 1, 0.0, 0)],\n",
       "  [(7, 1, 0.0, 0)],\n",
       "  [(9, 1, 0.0, 0)],\n",
       "  [(4, 0.25, 0.0, 1), (5, 0.75, 0.0, 1)],\n",
       "  [(6, 0.25, 1.0, 1), (2, 0.75, 1.0, 1)],\n",
       "  [(6, 0.25, 1.0, 1), (2, 0.75, 1.0, 1)],\n",
       "  []])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will probably not terminate when we remove truncation!\n",
    "def explore_policy(m, policy):\n",
    "    states = dict()\n",
    "    queue = set()\n",
    "\n",
    "    for s, p in m.start():\n",
    "        queue.add(s)\n",
    "\n",
    "    while len(queue) > 0:\n",
    "        s = queue.pop()\n",
    "\n",
    "        assert s not in states\n",
    "        states[s] = []\n",
    "\n",
    "        a = policy(s)\n",
    "        for t in m.apply(a, s):\n",
    "            if t.probability == 0:\n",
    "                continue\n",
    "\n",
    "            states[s].append((t.state, t.probability, t.reward, t.progress))\n",
    "\n",
    "            if t.state in states or t.state == terminal_state:\n",
    "                continue\n",
    "            else:\n",
    "                queue.add(t.state)\n",
    "\n",
    "    states[terminal_state] = []  # no transitions available in terminal_state\n",
    "\n",
    "    return m.start(), states\n",
    "\n",
    "\n",
    "def explore_honest(m):\n",
    "    return explore_policy(m, m.honest)\n",
    "\n",
    "\n",
    "def with_int_states(mc):\n",
    "    start, tx = mc\n",
    "    table = {s: i for i, s in enumerate(tx.keys())}\n",
    "    istart = []\n",
    "    for s0, p in start:\n",
    "        istart.append((table[s0], p))\n",
    "    istart = sorted(istart)\n",
    "    itx = []\n",
    "    for s0, tx_lst in tx.items():\n",
    "        itx.append([(table[s1], prb, rew, prg) for s1, prb, rew, prg in tx_lst])\n",
    "    return istart, itx\n",
    "\n",
    "\n",
    "mc = explore_honest(base_model)\n",
    "with_int_states(mc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216bdecf-3c58-44a4-a08a-3c3a290c6864",
   "metadata": {},
   "source": [
    "#### L0.2: evaluate MC; RTDP style\n",
    "\n",
    "- random walk through the MC\n",
    "- back-propagate values\n",
    "\n",
    "This should converge quickly initially as is focuses on important states.\n",
    "For the same reason it will slow down when getting closer to the true limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbc97e4a-2ab3-4beb-bdf9-1c4223131b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def sample_next_state_and_map(tx_lst, fun):\n",
    "    x = random.random()\n",
    "    i = None\n",
    "    acc_prb = 0.0\n",
    "    ret = []\n",
    "    for j, tx in enumerate(tx_lst):\n",
    "        ret.append(fun(tx))\n",
    "        if acc_prb <= x:\n",
    "            i = j\n",
    "        acc_prb += tx[1]\n",
    "    next_state = tx_lst[i][0]\n",
    "    return next_state, ret\n",
    "\n",
    "\n",
    "def mc_path_backpropagation_eval(m, policy):\n",
    "    start, tx = with_int_states(explore_policy(m, policy))\n",
    "\n",
    "    count = [0] * len(tx)\n",
    "    value = [0.0] * len(tx)\n",
    "    prg_v = [0.0] * len(tx)\n",
    "    state, _ = sample_next_state_and_map(start, lambda x: None)\n",
    "    terminating = False\n",
    "\n",
    "    n_steps = 100000\n",
    "    for i in range(n_steps):\n",
    "        if len(tx[state]) < 1:  # terminal state\n",
    "            terminating = True\n",
    "            state, _ = sample_next_state_and_map(start, lambda x: None)\n",
    "\n",
    "        def fun(t):\n",
    "            next_state, prb, rew, prg = t\n",
    "            return prb * (rew + value[next_state]), prb * (prg + prg_v[next_state])\n",
    "\n",
    "        next_state, values_and_prg_vs = sample_next_state_and_map(tx[state], fun)\n",
    "        values, prg_vs = zip(*values_and_prg_vs)\n",
    "        value[state] = sum(values)\n",
    "        prg_v[state] = sum(prg_vs)\n",
    "        count[next_state] += 1\n",
    "        state = next_state\n",
    "\n",
    "    # start state estimate\n",
    "    start_value_estimate = sum([value[s] * p for s, p in start])\n",
    "    start_prg_v_estimate = sum([prg_v[s] * p for s, p in start])\n",
    "\n",
    "    if terminating:\n",
    "        return start_value_estimate, start_prg_v_estimate, value, prg_v\n",
    "\n",
    "    else:\n",
    "        # steady state estimate\n",
    "        # note: steady state don't make sense for PTO; the terminal state is the steady state!\n",
    "        assert sum(count) == n_steps\n",
    "        steady_state = [c / n_steps for c in count]\n",
    "        steady_state_value = sum([v * c for v, c in zip(value, count)]) / n_steps\n",
    "        steady_state_prg_v = sum([v * c for v, c in zip(prg_v, count)]) / n_steps\n",
    "\n",
    "        return (\n",
    "            start_value_estimate,\n",
    "            start_prg_v_estimate,\n",
    "            value,\n",
    "            prg_v,\n",
    "            steady_state,\n",
    "            steady_state_value,\n",
    "            steady_state_prg_v,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "485c28b6-164a-4e8b-9576-12be25f1f81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2500000000000025, 0.25)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_reward, expected_progress, _, _ = mc_path_backpropagation_eval(\n",
    "    pto_model, pto_model.honest\n",
    ")\n",
    "assert abs(expected_progress - horizon) < 0.00001\n",
    "expected_reward / expected_progress, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9d08a1e-ad16-4918-b4b3-1d12a03cc2dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.24995653524277778, 0.25)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, _, _, _, _, expected_reward, expected_progress = mc_path_backpropagation_eval(\n",
    "    base_model, base_model.honest\n",
    ")\n",
    "expected_reward / expected_progress, alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec5f608-03f8-404b-9010-8ce98258bc5d",
   "metadata": {},
   "source": [
    "# Level 1: explore MDP around honest policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3af3fc3c-a632-4e4b-bf52-2d3861d837e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will probably not terminate when we remove common chain truncation in the selfish mining model!\n",
    "\n",
    "\n",
    "def explore_around_policy(m, policy, max_distance=0):\n",
    "    states = (\n",
    "        dict()\n",
    "    )  # state -> action -> list of transitions (next state, probability, reward, progress, ...)\n",
    "    queue = set()  # states to be explored\n",
    "    distances = dict()  # state -> distance to honest policy\n",
    "\n",
    "    def distance(s):\n",
    "        return distances.get(s, float(\"inf\"))\n",
    "\n",
    "    for s, p in m.start():\n",
    "        queue.add(s)\n",
    "        distances[s] = 0\n",
    "\n",
    "    while len(queue) > 0:\n",
    "        s = queue.pop()\n",
    "        d = distances[s]\n",
    "\n",
    "        assert s not in states\n",
    "        states[s] = dict()\n",
    "\n",
    "        h = policy(s)\n",
    "        for a in m.actions(s):\n",
    "            states[s][a] = []\n",
    "            for t in m.apply(a, s):\n",
    "                if t.probability == 0:\n",
    "                    continue\n",
    "\n",
    "                states[s][a].append(t)\n",
    "\n",
    "                if a == h:\n",
    "                    distances[t.state] = min(distance(t.state), distances[s])\n",
    "                else:\n",
    "                    distances[t.state] = min(distance(t.state), distances[s] + 1)\n",
    "                # I'm not sure about this mechanism\n",
    "                # What if we find a new path to a state with smaller distance than known before?\n",
    "                # What's the distance of state that can be reached with honest policy but only from a state with disctance > 0?\n",
    "                # I guess the right approach is incremental exploration: explore distance = 0; then = 1, and so on.\n",
    "                #\n",
    "                # Edit: This approach is certainly wrong. The resulting mdp size is not deterministic! It's fixed in the\n",
    "                # Explorer class below.\n",
    "\n",
    "                if distances[t.state] > max_distance:\n",
    "                    continue\n",
    "\n",
    "                if t.state in states or t.state == terminal_state:\n",
    "                    continue\n",
    "                else:\n",
    "                    queue.add(t.state)\n",
    "\n",
    "    states[terminal_state] = []  # no transitions available in terminal_state\n",
    "\n",
    "    return m.start(), states\n",
    "\n",
    "\n",
    "start, m = explore_around_policy(pto_model, pto_model.honest, max_distance=3)\n",
    "len(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "940a93db-e929-4a9e-9889-6756df466126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 11\n",
      "1 37 3.3636363636363638\n",
      "2 111 3.0\n",
      "3 279 2.5135135135135136\n",
      "4 601 2.154121863799283\n",
      "5 1166 1.940099833610649\n"
     ]
    }
   ],
   "source": [
    "# implementing the above loop as incremental exploration\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "# Invariant:\n",
    "# - policy-defined actions are explored first\n",
    "# - in the result MDP: policy = lambda s: 0\n",
    "class Explorer:\n",
    "    def __init__(self, model, policy):\n",
    "        self.model = model\n",
    "        self.policy = policy\n",
    "        self.partial_mdp = dict()  # state -> action -> transition list\n",
    "        self.policy_tab = dict()  # state -> action\n",
    "\n",
    "        self.unexplored = (\n",
    "            set()\n",
    "        )  # states relevant for the partially explored mdp but not yet explored at all\n",
    "        self.fully_explored = (\n",
    "            set()\n",
    "        )  # states in partial mdp for which all actions have been explored\n",
    "\n",
    "        for s, p in self.model.start():\n",
    "            self.unexplored.add(s)\n",
    "\n",
    "    def explore_along_policy(self):\n",
    "        queue = self.unexplored\n",
    "        states = self.partial_mdp\n",
    "\n",
    "        while len(queue) > 0:\n",
    "            s = queue.pop()\n",
    "\n",
    "            assert s not in states\n",
    "            states[s] = dict()\n",
    "\n",
    "            if len(self.model.actions(s)) == 0:\n",
    "                # s is a terminal state\n",
    "                continue\n",
    "\n",
    "            a = self.policy(s)\n",
    "            self.policy_tab[s] = a\n",
    "\n",
    "            states[s][a] = []\n",
    "            for t in self.model.apply(a, s):\n",
    "                if t.probability == 0:\n",
    "                    continue\n",
    "\n",
    "                states[s][a].append(t)\n",
    "\n",
    "                if t.state not in states:\n",
    "                    queue.add(t.state)\n",
    "\n",
    "    def explore_aside_policy(self):\n",
    "        self.explore_along_policy()\n",
    "        assert len(self.unexplored) == 0\n",
    "\n",
    "        states = self.partial_mdp\n",
    "\n",
    "        for s in set(self.partial_mdp.keys()) - self.fully_explored:\n",
    "            for a in self.model.actions(s):\n",
    "                if a in states:\n",
    "                    # honest action; already explored\n",
    "                    continue\n",
    "\n",
    "                states[s][a] = []\n",
    "                for t in self.model.apply(a, s):\n",
    "                    if t.probability == 0:\n",
    "                        continue\n",
    "\n",
    "                    states[s][a].append(t)\n",
    "\n",
    "                    if t.state not in states:\n",
    "                        self.unexplored.add(t.state)\n",
    "\n",
    "            self.fully_explored.add(s)\n",
    "\n",
    "    def mdp(self):\n",
    "        # Note 1. For some states in self.partial_mdp; we've only explored the honest action.\n",
    "        # That's okay, it forces the attacker to abort the attack.\n",
    "        # Note 2. Some states are reachable but not yet explored; we have to fix this before\n",
    "        # returning the MDP.\n",
    "        self.explore_along_policy()\n",
    "        assert len(self.unexplored) == 0\n",
    "\n",
    "        states = self.partial_mdp\n",
    "\n",
    "        def state_id(s):\n",
    "            if s in state_id.table:\n",
    "                return state_id.table[s]\n",
    "            else:\n",
    "                i = len(state_id.table)\n",
    "                state_id.table[s] = i\n",
    "                return i\n",
    "\n",
    "        state_id.table = dict()\n",
    "\n",
    "        m = mdp.MDP(start={state_id(s): prb for s, prb in self.model.start()})\n",
    "\n",
    "        for s, actions in states.items():\n",
    "\n",
    "            def per_action(i, a):\n",
    "                for t in actions[a]:\n",
    "                    it = mdp.Transition(\n",
    "                        probability=t.probability,\n",
    "                        destination=state_id(t.state),\n",
    "                        reward=t.reward,\n",
    "                        progress=t.progress,\n",
    "                        effect=t.effect,\n",
    "                    )\n",
    "                    m.add_transition(src=state_id(s), act=i, t=it)\n",
    "\n",
    "            if len(actions) == 0:\n",
    "                continue\n",
    "\n",
    "            policy_a = self.policy_tab[s]\n",
    "            per_action(0, policy_a)  # ensure policy = lambda _: 0\n",
    "            for i, a in enumerate(set(actions.keys()) - {policy_a}):\n",
    "                per_action(i + 1, a)\n",
    "\n",
    "        m.check()\n",
    "\n",
    "        return m\n",
    "\n",
    "\n",
    "e = Explorer(pto_model, pto_model.honest)\n",
    "d = 0\n",
    "m = e.mdp()\n",
    "ms = [m]\n",
    "n = e.mdp().n_states\n",
    "print(d, n)\n",
    "for i in range(5):\n",
    "    e.explore_aside_policy()\n",
    "    d += 1\n",
    "    m = e.mdp()\n",
    "    ms.append(m)\n",
    "    prev_n = n\n",
    "    n = m.n_states\n",
    "    print(d, n, n / prev_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770ccec8-4a72-4fc1-9042-53383efcd924",
   "metadata": {},
   "source": [
    "# Level 2: optimize the MDPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0c91575-915a-48e0-b3ee-8f3d43f4c644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 11 True 99.9960163960487 0.25000000000000205\n",
      "1 37 True 99.99605623208821 0.25000000000000205\n",
      "2 111 False 99.99609566976731 0.25000000000000205\n",
      "3 279 False 99.99609566976731 0.25000000000000205\n",
      "4 601 False 99.99613471306962 0.25000000000000205\n",
      "5 1166 False 99.99613471306962 0.25000000000000205\n"
     ]
    }
   ],
   "source": [
    "def evaluate(m):\n",
    "    res = m.value_iteration(stop_delta=0.00001)\n",
    "    res[\"strictly_honest\"] = all(a < 1 for a in res[\"vi_policy\"])\n",
    "    res[\"episode_reward\"] = sum(res[\"vi_value\"][s] * p for s, p in m.start.items())\n",
    "    res[\"episode_progress\"] = sum(res[\"vi_progress\"][s] * p for s, p in m.start.items())\n",
    "    res[\"reward_per_progress\"] = res[\"episode_reward\"] / res[\"episode_progress\"]\n",
    "    return res\n",
    "\n",
    "\n",
    "for d, m in enumerate(ms):\n",
    "    res = evaluate(m)\n",
    "    print(\n",
    "        d,\n",
    "        m.n_states,\n",
    "        res[\"strictly_honest\"],\n",
    "        res[\"episode_progress\"],\n",
    "        res[\"reward_per_progress\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7a2365-3ba2-4d3c-81e4-c6b528273f1e",
   "metadata": {},
   "source": [
    "It comes unexpected that the `res['vi_policy']` is dishonest on some points whithout the policy yielding excess reward.\n",
    "\n",
    "Is this maybe on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "febd7d36-82c2-4447-9e7b-7b718aa76f93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(x > 0 for x in res[\"vi_policy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f31b1d-b9df-4712-bc8f-b18d5f4bb6a4",
   "metadata": {},
   "source": [
    "# Level 3: Find break-even points"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
