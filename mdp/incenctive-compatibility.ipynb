{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e82b267-af0c-4b9f-9a8b-16c8650b08ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bitcoin import Bitcoin\n",
    "import model\n",
    "import sm\n",
    "import mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfb7b3cb-b009-4eee-81f6-26417a5b000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "protocol = Bitcoin()\n",
    "alpha = 0.25\n",
    "gamma = 0.25\n",
    "\n",
    "# implicit SM model\n",
    "# careful: this truncates the common chain; that mechanism is easy to define for Bitcoin but might cause headaches for other protocols\n",
    "# take in mind that the (absence of) truncation might affect the rest of the algorithm\n",
    "base_model = sm.SelfishMining(protocol, alpha=alpha, gamma=gamma, maximum_size=1000000)\n",
    "\n",
    "terminal_state = b\"\"\n",
    "horizon = 100\n",
    "\n",
    "# implicit SM model with PTO applied on the fly\n",
    "pto_model = model.PTO_wrapper(\n",
    "    base_model, horizon=horizon, terminal_state=terminal_state\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751a8334-06ad-4d84-8ae4-a1d11fe3f9f9",
   "metadata": {},
   "source": [
    "# Level 0: explore honest policy, derive MC, evaluate\n",
    "\n",
    "#### L0.1: explore states and build MC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e909d654-d09a-4afa-820b-2dd67f3b91e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(0, 0.75), (1, 0.25)],\n",
       " [[(3, 0.0625, 0.0, 0),\n",
       "   (5, 0.1875, 0.0, 0),\n",
       "   (3, 0.1875, 0.0, 0),\n",
       "   (5, 0.5625, 0.0, 0)],\n",
       "  [(4, 0.0625, 0.0, 0),\n",
       "   (2, 0.1875, 0.0, 0),\n",
       "   (4, 0.1875, 0.0, 0),\n",
       "   (2, 0.5625, 0.0, 0)],\n",
       "  [(7, 1, 0.0, 0)],\n",
       "  [(6, 1, 0.0, 0)],\n",
       "  [(8, 1, 0.0, 0)],\n",
       "  [(9, 1, 0.0, 0)],\n",
       "  [(4, 0.0625, 1.0, 1),\n",
       "   (2, 0.1875, 1.0, 1),\n",
       "   (4, 0.1875, 1.0, 1),\n",
       "   (2, 0.5625, 1.0, 1)],\n",
       "  [(3, 0.0625, 0.0, 1),\n",
       "   (5, 0.1875, 0.0, 1),\n",
       "   (3, 0.1875, 0.0, 1),\n",
       "   (5, 0.5625, 0.0, 1)],\n",
       "  [(4, 0.0625, 1.0, 1),\n",
       "   (2, 0.1875, 1.0, 1),\n",
       "   (4, 0.1875, 1.0, 1),\n",
       "   (2, 0.5625, 1.0, 1)],\n",
       "  [(3, 0.0625, 0.0, 1),\n",
       "   (5, 0.1875, 0.0, 1),\n",
       "   (3, 0.1875, 0.0, 1),\n",
       "   (5, 0.5625, 0.0, 1)],\n",
       "  []])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will probably not terminate when we remove truncation!\n",
    "def explore_policy(m, policy):\n",
    "    states = dict()\n",
    "    queue = set()\n",
    "\n",
    "    for s, p in m.start():\n",
    "        queue.add(s)\n",
    "\n",
    "    while len(queue) > 0:\n",
    "        s = queue.pop()\n",
    "\n",
    "        assert s not in states\n",
    "        states[s] = []\n",
    "\n",
    "        a = policy(s)\n",
    "        for t in m.apply(a, s):\n",
    "            if t.probability == 0:\n",
    "                continue\n",
    "\n",
    "            states[s].append((t.state, t.probability, t.reward, t.progress))\n",
    "\n",
    "            if t.state in states or t.state == terminal_state:\n",
    "                continue\n",
    "            else:\n",
    "                queue.add(t.state)\n",
    "\n",
    "    states[terminal_state] = []  # no transitions available in terminal_state\n",
    "\n",
    "    return m.start(), states\n",
    "\n",
    "\n",
    "def explore_honest(m):\n",
    "    return explore_policy(m, m.honest)\n",
    "\n",
    "\n",
    "def with_int_states(mc):\n",
    "    start, tx = mc\n",
    "    table = {s: i for i, s in enumerate(tx.keys())}\n",
    "    istart = []\n",
    "    for s0, p in start:\n",
    "        istart.append((table[s0], p))\n",
    "    istart = sorted(istart)\n",
    "    itx = []\n",
    "    for s0, tx_lst in tx.items():\n",
    "        itx.append([(table[s1], prb, rew, prg) for s1, prb, rew, prg in tx_lst])\n",
    "    return istart, itx\n",
    "\n",
    "\n",
    "mc = explore_honest(base_model)\n",
    "with_int_states(mc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216bdecf-3c58-44a4-a08a-3c3a290c6864",
   "metadata": {},
   "source": [
    "#### L0.2: evaluate MC; RTDP style\n",
    "\n",
    "- random walk through the MC\n",
    "- back-propagate values\n",
    "\n",
    "This should converge quickly initially as is focuses on important states.\n",
    "For the same reason it will slow down when getting closer to the true limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbc97e4a-2ab3-4beb-bdf9-1c4223131b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def sample_next_state_and_map(tx_lst, fun):\n",
    "    x = random.random()\n",
    "    i = None\n",
    "    acc_prb = 0.0\n",
    "    ret = []\n",
    "    for j, tx in enumerate(tx_lst):\n",
    "        ret.append(fun(tx))\n",
    "        if acc_prb <= x:\n",
    "            i = j\n",
    "        acc_prb += tx[1]\n",
    "    next_state = tx_lst[i][0]\n",
    "    return next_state, ret\n",
    "\n",
    "\n",
    "def mc_path_backpropagation_eval(m, policy):\n",
    "    start, tx = with_int_states(explore_policy(m, policy))\n",
    "\n",
    "    count = [0] * len(tx)\n",
    "    value = [0.0] * len(tx)\n",
    "    prg_v = [0.0] * len(tx)\n",
    "    state, _ = sample_next_state_and_map(start, lambda x: None)\n",
    "    terminating = False\n",
    "\n",
    "    n_steps = 100000\n",
    "    for i in range(n_steps):\n",
    "        if len(tx[state]) < 1:  # terminal state\n",
    "            terminating = True\n",
    "            state, _ = sample_next_state_and_map(start, lambda x: None)\n",
    "\n",
    "        def fun(t):\n",
    "            next_state, prb, rew, prg = t\n",
    "            return prb * (rew + value[next_state]), prb * (prg + prg_v[next_state])\n",
    "\n",
    "        next_state, values_and_prg_vs = sample_next_state_and_map(tx[state], fun)\n",
    "        values, prg_vs = zip(*values_and_prg_vs)\n",
    "        value[state] = sum(values)\n",
    "        prg_v[state] = sum(prg_vs)\n",
    "        count[next_state] += 1\n",
    "        state = next_state\n",
    "\n",
    "    # start state estimate\n",
    "    start_value_estimate = sum([value[s] * p for s, p in start])\n",
    "    start_prg_v_estimate = sum([prg_v[s] * p for s, p in start])\n",
    "\n",
    "    if terminating:\n",
    "        return start_value_estimate, start_prg_v_estimate, value, prg_v\n",
    "\n",
    "    else:\n",
    "        # steady state estimate\n",
    "        # note: steady state don't make sense for PTO; the terminal state is the steady state!\n",
    "        assert sum(count) == n_steps\n",
    "        steady_state = [c / n_steps for c in count]\n",
    "        steady_state_value = sum([v * c for v, c in zip(value, count)]) / n_steps\n",
    "        steady_state_prg_v = sum([v * c for v, c in zip(prg_v, count)]) / n_steps\n",
    "\n",
    "        return (\n",
    "            start_value_estimate,\n",
    "            start_prg_v_estimate,\n",
    "            value,\n",
    "            prg_v,\n",
    "            steady_state,\n",
    "            steady_state_value,\n",
    "            steady_state_prg_v,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "485c28b6-164a-4e8b-9576-12be25f1f81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.24999999999999736, 0.25)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_reward, expected_progress, _, _ = mc_path_backpropagation_eval(\n",
    "    pto_model, pto_model.honest\n",
    ")\n",
    "assert abs(expected_progress - horizon) < 0.00001\n",
    "expected_reward / expected_progress, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9d08a1e-ad16-4918-b4b3-1d12a03cc2dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2499884858138771, 0.25)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, _, _, _, _, expected_reward, expected_progress = mc_path_backpropagation_eval(\n",
    "    base_model, base_model.honest\n",
    ")\n",
    "expected_reward / expected_progress, alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec5f608-03f8-404b-9010-8ce98258bc5d",
   "metadata": {},
   "source": [
    "# Level 1: explore MDP around honest policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3af3fc3c-a632-4e4b-bf52-2d3861d837e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will probably not terminate when we remove common chain truncation in the selfish mining model!\n",
    "\n",
    "\n",
    "def explore_around_policy(m, policy, max_distance=0):\n",
    "    states = (\n",
    "        dict()\n",
    "    )  # state -> action -> list of transitions (next state, probability, reward, progress, ...)\n",
    "    queue = set()  # states to be explored\n",
    "    distances = dict()  # state -> distance to honest policy\n",
    "\n",
    "    def distance(s):\n",
    "        return distances.get(s, float(\"inf\"))\n",
    "\n",
    "    for s, p in m.start():\n",
    "        queue.add(s)\n",
    "        distances[s] = 0\n",
    "\n",
    "    while len(queue) > 0:\n",
    "        s = queue.pop()\n",
    "        d = distances[s]\n",
    "\n",
    "        assert s not in states\n",
    "        states[s] = dict()\n",
    "\n",
    "        h = policy(s)\n",
    "        for a in m.actions(s):\n",
    "            states[s][a] = []\n",
    "            for t in m.apply(a, s):\n",
    "                if t.probability == 0:\n",
    "                    continue\n",
    "\n",
    "                states[s][a].append(t)\n",
    "\n",
    "                if a == h:\n",
    "                    distances[t.state] = min(distance(t.state), distances[s])\n",
    "                else:\n",
    "                    distances[t.state] = min(distance(t.state), distances[s] + 1)\n",
    "                # I'm not sure about this mechanism\n",
    "                # What if we find a new path to a state with smaller distance than known before?\n",
    "                # What's the distance of state that can be reached with honest policy but only from a state with disctance > 0?\n",
    "                # I guess the right approach is incremental exploration: explore distance = 0; then = 1, and so on.\n",
    "                #\n",
    "                # Edit: This approach is certainly wrong. The resulting mdp size is not deterministic! It's fixed in the\n",
    "                # Explorer class below.\n",
    "\n",
    "                if distances[t.state] > max_distance:\n",
    "                    continue\n",
    "\n",
    "                if t.state in states or t.state == terminal_state:\n",
    "                    continue\n",
    "                else:\n",
    "                    queue.add(t.state)\n",
    "\n",
    "    states[terminal_state] = []  # no transitions available in terminal_state\n",
    "\n",
    "    return m.start(), states\n",
    "\n",
    "\n",
    "start, m = explore_around_policy(pto_model, pto_model.honest, max_distance=3)\n",
    "len(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "940a93db-e929-4a9e-9889-6756df466126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 11\n",
      "1 37 3.3636363636363638\n",
      "2 111 3.0\n",
      "3 279 2.5135135135135136\n",
      "4 601 2.154121863799283\n",
      "5 1166 1.940099833610649\n"
     ]
    }
   ],
   "source": [
    "# implementing the above loop as incremental exploration\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "# Invariant:\n",
    "# - policy-defined actions are explored first\n",
    "# - in the result MDP: policy = lambda s: 0\n",
    "# - states are enumerated in order of exploration\n",
    "# - states far off the policy have higher state ids than state closer or on the policy\n",
    "# - policies for smaller MDPs are compatible with policies for bigger MDPs\n",
    "class Explorer:\n",
    "    def __init__(self, model, policy):\n",
    "        self.model = model\n",
    "        self.policy = policy\n",
    "\n",
    "        self._mdp = mdp.MDP()\n",
    "        self.states = []  # state id int -> state\n",
    "        self.policy_tab = []  # state id int -> action\n",
    "\n",
    "        self._state_id = dict()  # state -> state id int; see self.state_id()\n",
    "        self.explored_upto = -1  # state id int; policy action has been explored\n",
    "        self.fully_explored_upto = -1  # state id int; all actions have been explored\n",
    "\n",
    "        for s, p in self.model.start():\n",
    "            self._mdp.start[self.state_id(s)] = p\n",
    "\n",
    "    def state_id(self, state):\n",
    "        if state in self._state_id:\n",
    "            return self._state_id[state]\n",
    "        else:\n",
    "            i = len(self._state_id)\n",
    "            self._state_id[state] = i\n",
    "            self.states.append(state)\n",
    "            return i\n",
    "\n",
    "    @property\n",
    "    def n_states(self):\n",
    "        return len(self._state_id)\n",
    "\n",
    "    @property\n",
    "    def max_state_id(self):\n",
    "        return len(self._state_id) - 1\n",
    "\n",
    "    def explore_along_policy(self):\n",
    "        while self.max_state_id > self.explored_upto:\n",
    "            self.explored_upto += 1\n",
    "            s_id = self.explored_upto\n",
    "            s = self.states[s_id]\n",
    "\n",
    "            assert (\n",
    "                len(self.policy_tab) == s_id\n",
    "            ), f\"logic error, {len(self.policy_tab)} == {s_id}\"\n",
    "\n",
    "            if len(self.model.actions(s)) == 0:\n",
    "                # s is a terminal state\n",
    "                self.policy_tab.append(-1)  # policy is not defined, put placeholder\n",
    "                continue\n",
    "\n",
    "            a = self.policy(s)\n",
    "            self.policy_tab.append(a)\n",
    "\n",
    "            for t in self.model.apply(a, s):\n",
    "                if t.probability == 0:\n",
    "                    continue\n",
    "\n",
    "                t = mdp.Transition(\n",
    "                    probability=t.probability,\n",
    "                    destination=self.state_id(t.state),\n",
    "                    reward=t.reward,\n",
    "                    progress=t.progress,\n",
    "                    effect=t.effect,\n",
    "                )\n",
    "                self._mdp.add_transition(src=s_id, act=0, t=t)\n",
    "\n",
    "            self.explored_upto = s_id\n",
    "\n",
    "    def explore_aside_policy(self):\n",
    "        self.explore_along_policy()\n",
    "        assert self.explored_upto == self.max_state_id\n",
    "\n",
    "        while self.fully_explored_upto < self.explored_upto:\n",
    "            self.fully_explored_upto += 1\n",
    "            s_id = self.fully_explored_upto\n",
    "            s = self.states[s_id]\n",
    "\n",
    "            a_idx = 0  # policy action has a_idx = 0\n",
    "            for a in self.model.actions(s):\n",
    "                if a == self.policy_tab[s_id]:\n",
    "                    # policy action; already explored\n",
    "                    continue\n",
    "\n",
    "                a_idx += 1\n",
    "\n",
    "                for t in self.model.apply(a, s):\n",
    "                    if t.probability == 0:\n",
    "                        continue\n",
    "\n",
    "                    t = mdp.Transition(\n",
    "                        probability=t.probability,\n",
    "                        destination=self.state_id(t.state),\n",
    "                        reward=t.reward,\n",
    "                        progress=t.progress,\n",
    "                        effect=t.effect,\n",
    "                    )\n",
    "                    self._mdp.add_transition(src=s_id, act=a_idx, t=t)\n",
    "\n",
    "    def mdp(self):\n",
    "        # Note 1. For some states in self.partial_mdp; we've only explored the honest action.\n",
    "        # That's okay, it forces the attacker to abort the attack.\n",
    "        # Note 2. Some states are reachable but not yet explored; we have to fix this before\n",
    "        # returning the MDP.\n",
    "        self.explore_along_policy()\n",
    "        assert self.explored_upto == self.max_state_id\n",
    "\n",
    "        self._mdp.check()\n",
    "\n",
    "        return deepcopy(self._mdp)\n",
    "\n",
    "\n",
    "e = Explorer(pto_model, pto_model.honest)\n",
    "d = 0\n",
    "m = e.mdp()\n",
    "ms = [m]\n",
    "n = m.n_states\n",
    "print(d, n)\n",
    "for i in range(5):\n",
    "    e.explore_aside_policy()\n",
    "    d += 1\n",
    "    m = e.mdp()\n",
    "    ms.append(m)\n",
    "    prev_n = n\n",
    "    n = m.n_states\n",
    "    print(d, n, n / prev_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770ccec8-4a72-4fc1-9042-53383efcd924",
   "metadata": {},
   "source": [
    "# Level 2: optimize the MDPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0c91575-915a-48e0-b3ee-8f3d43f4c644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 11 True True 99.99601639605103 0.2499999999999963\n",
      "1 37 True True 99.99605623209052 0.2499999999999963\n",
      "2 111 False True 99.99609566976963 0.2499999999999963\n",
      "3 279 False True 99.99609566976963 0.2499999999999963\n",
      "4 601 False True 99.99613471307195 0.24999999999999628\n",
      "5 1166 False True 99.99613471307195 0.24999999999999628\n"
     ]
    }
   ],
   "source": [
    "def evaluate(m, m_honest):\n",
    "    res = m.value_iteration(stop_delta=0.00001)\n",
    "    res[\"episode_reward\"] = sum(res[\"vi_value\"][s] * p for s, p in m.start.items())\n",
    "    res[\"episode_progress\"] = sum(res[\"vi_progress\"][s] * p for s, p in m.start.items())\n",
    "    res[\"reward_per_progress\"] = res[\"episode_reward\"] / res[\"episode_progress\"]\n",
    "\n",
    "    res[\"fully_honest\"] = all(a < 1 for a in res[\"vi_policy\"])\n",
    "    # It at first came unexpeted to me, that vi_policy has dishonest entries while the honest policy was optimal.\n",
    "    # Turns out the dishonest behavior happens on states that cannot be reached from the honest policy.\n",
    "    # I'm now comparing the policies for only these states which can be rewach from the honest policy.\n",
    "    # If the agent acts honest on these states it acts honest everywhere.\n",
    "    res[\"honest\"] = all(a < 1 for a in res[\"vi_policy\"][: m_honest.n_states])\n",
    "    return res\n",
    "\n",
    "\n",
    "for d, m in enumerate(ms):\n",
    "    res = evaluate(m, ms[0])\n",
    "    print(\n",
    "        d,\n",
    "        m.n_states,\n",
    "        res[\"fully_honest\"],\n",
    "        res[\"honest\"],\n",
    "        res[\"episode_progress\"],\n",
    "        res[\"reward_per_progress\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f31b1d-b9df-4712-bc8f-b18d5f4bb6a4",
   "metadata": {},
   "source": [
    "# Level 3: Find break-even points"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
