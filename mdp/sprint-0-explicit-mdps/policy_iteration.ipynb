{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab192e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from aft20barzur import BitcoinSM, map_params, mappable_params, ptmdp\n",
    "from compiler import Compiler\n",
    "from time import time\n",
    "import sm\n",
    "import bitcoin\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a775a810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MDP of size 1624 / 4 / 6318 / 3.9"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if False:\n",
    "    mdp = Compiler(\n",
    "        sm.SelfishMining(bitcoin.Bitcoin(), **sm.mappable_params, maximum_height=6)\n",
    "    ).mdp()\n",
    "    mdp = sm.map_params(mdp, alpha=0.33, gamma=0.75)\n",
    "else:\n",
    "    mdp = Compiler(BitcoinSM(**mappable_params, maximum_fork_length=25)).mdp()\n",
    "    mdp = map_params(mdp, alpha=0.33, gamma=0.75)\n",
    "mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b0a3c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTO revenue (value iteration): 41.20263153697032\n"
     ]
    }
   ],
   "source": [
    "# benchmark: PTO + value iteration\n",
    "pmdp = ptmdp(mdp, horizon=100)\n",
    "vi = pmdp.value_iteration(stop_delta=0.001)\n",
    "\n",
    "vi_ss = mdp.steady_state(vi[\"vi_policy\"], start_state=numpy.argmax(\"vi_value\"))[\"ss\"]\n",
    "\n",
    "vi_revenue = vi[\"vi_value\"][:-1].dot(vi_ss)\n",
    "print(\"PTO revenue (value iteration):\", vi_revenue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e728fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTO revenue (value iteration):   41.20263153697032\n",
      "PTO revenue (policy evaluation): 41.20285764729448\n"
     ]
    }
   ],
   "source": [
    "# wip: PTO + policy iteration (policy evaluation only)\n",
    "def policy_evaluation(self, policy, *args, theta, discount=1, verbose=False):\n",
    "    value = numpy.zeros((2, self.n_states), dtype=float)\n",
    "\n",
    "    i = 1\n",
    "    while True:\n",
    "        prev = i % 2\n",
    "        next = (prev + 1) % 2\n",
    "\n",
    "        for src, actions in enumerate(self.tab):\n",
    "            a = policy[src]\n",
    "            if a < 0:\n",
    "                continue\n",
    "            v = 0.0\n",
    "            for t in actions[a]:\n",
    "                v += t.probability * (t.reward + discount * value[prev, t.destination])\n",
    "            value[next, src] = v\n",
    "\n",
    "        delta = numpy.abs(value[next,] - value[prev,]).max()\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\riteration {i}: delta {delta:g}\")\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return value[next,]\n",
    "\n",
    "\n",
    "pe_value = policy_evaluation(pmdp, vi[\"vi_policy\"], theta=0.001)\n",
    "pe_revenue = pe_value[:-1].dot(vi_ss)\n",
    "print(\"PTO revenue (value iteration):  \", vi_revenue)\n",
    "print(\"PTO revenue (policy evaluation):\", pe_revenue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "333c550e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTO revenue (value iteration):      41.20263153697032\n",
      "PTO revenue (policy evaluation):    41.20285764729448\n",
      "PTO revenue (policy evaluation ro): 41.20285764729448\n"
     ]
    }
   ],
   "source": [
    "# wip: PTO + policy iteration (policy evaluation only) (reachable only)\n",
    "def policy_evaluation_ro(\n",
    "    self, policy, *args, theta, discount=1, verbose=False, start_state=None\n",
    "):\n",
    "    value = numpy.zeros((2, self.n_states), dtype=float)\n",
    "\n",
    "    reachable = self.reachable_states(policy, start_state=start_state)\n",
    "\n",
    "    i = 1\n",
    "    while True:\n",
    "        prev = i % 2\n",
    "        next = (prev + 1) % 2\n",
    "\n",
    "        for src in reachable:\n",
    "            a = policy[src]\n",
    "            if a < 0:\n",
    "                continue\n",
    "            v = 0.0\n",
    "            for t in self.tab[src][a]:\n",
    "                v += t.probability * (t.reward + discount * value[prev, t.destination])\n",
    "            value[next, src] = v\n",
    "\n",
    "        delta = numpy.abs(value[next,] - value[prev,]).max()\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\riteration {i}: delta {delta:g}\")\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return value[next,]\n",
    "\n",
    "\n",
    "pero_value = policy_evaluation_ro(pmdp, vi[\"vi_policy\"], theta=0.001)\n",
    "pero_revenue = pero_value[:-1].dot(vi_ss)\n",
    "print(\"PTO revenue (value iteration):     \", vi_revenue)\n",
    "print(\"PTO revenue (policy evaluation):   \", pe_revenue)\n",
    "print(\"PTO revenue (policy evaluation ro):\", pero_revenue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "471fa46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTO revenue (value iteration):      41.20263153697032\n",
      "PTO revenue (policy evaluation):    41.20285764729448\n",
      "PTO revenue (policy evaluation ro): 41.20285764729448\n",
      "PTO revenue (policy iteration):     41.20285764729448\n",
      "PTO revenue (policy iteration ro):  32.90149018623927\n"
     ]
    }
   ],
   "source": [
    "# wip: PTO + policy iteration\n",
    "def policy_iteration(\n",
    "    self, *args, theta, discount=1, verbose=False, reachable_only=True\n",
    "):\n",
    "    start = time()\n",
    "\n",
    "    policy = numpy.full(self.n_states, -1, dtype=int)\n",
    "\n",
    "    if reachable_only:\n",
    "        best_state = None\n",
    "\n",
    "    i = 1\n",
    "    while True:\n",
    "        stable = True\n",
    "\n",
    "        if reachable_only:\n",
    "            value = policy_evaluation_ro(\n",
    "                self, policy, theta=theta, discount=discount, start_state=best_state\n",
    "            )\n",
    "            best_state = numpy.argmax(value)\n",
    "        else:\n",
    "            value = policy_evaluation(self, policy, theta=theta, discount=discount)\n",
    "\n",
    "        for src, actions in enumerate(self.tab):\n",
    "            best_v = float(\"-inf\")\n",
    "            best_a = -1  # no action possible\n",
    "            for a, lst in actions.items():\n",
    "                if a < 0:\n",
    "                    continue\n",
    "                v = 0.0\n",
    "                for t in lst:\n",
    "                    v += t.probability * (t.reward + discount * value[t.destination])\n",
    "                if v > best_v:\n",
    "                    best_v = v\n",
    "                    best_a = a\n",
    "\n",
    "            if policy[src] != best_a:\n",
    "                stable = False\n",
    "\n",
    "            policy[src] = best_a\n",
    "\n",
    "        if stable:\n",
    "            break\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return dict(pi_value=value, pi_policy=policy, pi_iter=i, pi_time=time() - start)\n",
    "\n",
    "\n",
    "pi = policy_iteration(pmdp, theta=0.001, reachable_only=False)\n",
    "pi_value = pi.pop(\"pi_value\")\n",
    "pi_ss = mdp.steady_state(pi[\"pi_policy\"], start_state=numpy.argmax(pi_value))[\"ss\"]\n",
    "pi_revenue = pi_value[:-1].dot(pi_ss)\n",
    "\n",
    "piro = policy_iteration(pmdp, theta=0.001, reachable_only=True)\n",
    "piro_value = piro.pop(\"pi_value\")\n",
    "piro_ss = mdp.steady_state(piro[\"pi_policy\"], start_state=numpy.argmax(piro_value))[\n",
    "    \"ss\"\n",
    "]\n",
    "piro_revenue = piro_value[:-1].dot(piro_ss)\n",
    "print(\"PTO revenue (value iteration):     \", vi_revenue)\n",
    "print(\"PTO revenue (policy evaluation):   \", pe_revenue)\n",
    "print(\"PTO revenue (policy evaluation ro):\", pero_revenue)\n",
    "print(\"PTO revenue (policy iteration):    \", pi_revenue)\n",
    "print(\"PTO revenue (policy iteration ro): \", piro_revenue)\n",
    "\n",
    "# conclude: reachable-only does work for evaluation but not for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8d1ea45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTO revenue (value iteration):      41.20263153697032 4.203214645385742\n",
      "PTO revenue (policy evaluation):    41.20285764729448\n",
      "PTO revenue (policy evaluation ro): 41.20285764729448\n",
      "PTO revenue (policy iteration):     41.20285764729448 7.576725721359253\n",
      "PTO revenue (policy iteration rvf): 41.20286480649146 3.887073516845703\n"
     ]
    }
   ],
   "source": [
    "# wip: PTO + policy iteration (reuse value function)\n",
    "def policy_evaluation_rvf(\n",
    "    self, policy, *args, theta, discount=1, verbose=False, init=None\n",
    "):\n",
    "    value = numpy.zeros((2, self.n_states), dtype=float)\n",
    "\n",
    "    if init is not None:\n",
    "        value[0,] = init\n",
    "        value[1,] = init\n",
    "\n",
    "    i = 1\n",
    "    while True:\n",
    "        prev = i % 2\n",
    "        next = (prev + 1) % 2\n",
    "\n",
    "        for src, actions in enumerate(self.tab):\n",
    "            a = policy[src]\n",
    "            if a < 0:\n",
    "                continue\n",
    "            v = 0.0\n",
    "            for t in actions[a]:\n",
    "                v += t.probability * (t.reward + discount * value[prev, t.destination])\n",
    "            value[next, src] = v\n",
    "\n",
    "        delta = numpy.abs(value[next,] - value[prev,]).max()\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\riteration {i}: delta {delta:g}\")\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return value[next,]\n",
    "\n",
    "\n",
    "def policy_iteration_rvf(self, *args, theta, discount=1, verbose=False):\n",
    "    start = time()\n",
    "\n",
    "    policy = numpy.full(self.n_states, -1, dtype=int)\n",
    "    value = policy_evaluation_rvf(self, policy, theta=theta, discount=discount)\n",
    "\n",
    "    i = 1\n",
    "    while True:\n",
    "        stable = True\n",
    "\n",
    "        for src, actions in enumerate(self.tab):\n",
    "            best_v = float(\"-inf\")\n",
    "            best_a = -1  # no action possible\n",
    "            for a, lst in actions.items():\n",
    "                if a < 0:\n",
    "                    continue\n",
    "                v = 0.0\n",
    "                for t in lst:\n",
    "                    v += t.probability * (t.reward + discount * value[t.destination])\n",
    "                if v > best_v:\n",
    "                    best_v = v\n",
    "                    best_a = a\n",
    "\n",
    "            if policy[src] != best_a:\n",
    "                stable = False\n",
    "\n",
    "            policy[src] = best_a\n",
    "\n",
    "        if stable:\n",
    "            break\n",
    "\n",
    "        i += 1\n",
    "        value = policy_evaluation_rvf(\n",
    "            self, policy, theta=theta, discount=discount, init=value\n",
    "        )\n",
    "\n",
    "    return dict(pi_value=value, pi_policy=policy, pi_iter=i, pi_time=time() - start)\n",
    "\n",
    "\n",
    "pirvf = policy_iteration_rvf(pmdp, theta=0.001)\n",
    "pirvf_value = pirvf.pop(\"pi_value\")\n",
    "pirvf_ss = mdp.steady_state(pirvf[\"pi_policy\"], start_state=numpy.argmax(pirvf_value))[\n",
    "    \"ss\"\n",
    "]\n",
    "pirvf_revenue = pirvf_value[:-1].dot(pirvf_ss)\n",
    "print(\"PTO revenue (value iteration):     \", vi_revenue, vi[\"vi_time\"])\n",
    "print(\"PTO revenue (policy evaluation):   \", pe_revenue)\n",
    "print(\"PTO revenue (policy evaluation ro):\", pero_revenue)\n",
    "print(\"PTO revenue (policy iteration):    \", pi_revenue, pi[\"pi_time\"])\n",
    "print(\"PTO revenue (policy iteration rvf):\", pirvf_revenue, pirvf[\"pi_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b56a5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RPP (policy iteration) (this cell): 0.40428299140506413\n"
     ]
    }
   ],
   "source": [
    "# Does reward per progress do what it should?\n",
    "def policy_evaluation_full(\n",
    "    self,\n",
    "    policy,\n",
    "    *args,\n",
    "    theta,\n",
    "    discount=1,\n",
    "    verbose=False,\n",
    "    around_state=None,\n",
    "    max_iter=None,\n",
    "):\n",
    "    rew = numpy.zeros((2, self.n_states), dtype=float)\n",
    "    prg = numpy.zeros((2, self.n_states), dtype=float)\n",
    "\n",
    "    if around_state is None:\n",
    "        included_states = self.reachable_states(policy, start_state=around_state)\n",
    "    else:\n",
    "        included_states = range(self.n_states)\n",
    "\n",
    "    i = 1\n",
    "    while True:\n",
    "        prev = i % 2\n",
    "        next = (prev + 1) % 2\n",
    "\n",
    "        for src in included_states:\n",
    "            a = policy[src]\n",
    "            if a < 0:\n",
    "                continue\n",
    "            r = 0.0\n",
    "            p = 0.0\n",
    "            for t in self.tab[src][a]:\n",
    "                r += t.probability * (t.reward + discount * rew[prev, t.destination])\n",
    "                p += t.probability * (t.progress + discount * prg[prev, t.destination])\n",
    "            rew[next, src] = r\n",
    "            prg[next, src] = p\n",
    "\n",
    "        delta = numpy.abs(rew[next,] - rew[prev,]).max()\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\riteration {i}: delta {delta:g}\")\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "        if max_iter is not None and i >= max_iter:\n",
    "            break\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return dict(pe_reward=rew[next,], pe_progress=prg[next,], pe_iter=i)\n",
    "\n",
    "\n",
    "best_state = numpy.argmax(pirvf_value)\n",
    "# Evaluate policy in PTO space, note number of iterations\n",
    "ppe = policy_evaluation_full(\n",
    "    pmdp, pirvf[\"pi_policy\"], around_state=best_state, theta=0.001\n",
    ")\n",
    "# Evalutate policy in divergent space for same number of iterations\n",
    "pe = policy_evaluation_full(\n",
    "    mdp, pirvf[\"pi_policy\"], around_state=best_state, theta=0, max_iter=ppe[\"pe_iter\"]\n",
    ")\n",
    "pe_prg = pe[\"pe_progress\"].dot(pirvf_ss)\n",
    "pe_rew = pe[\"pe_reward\"].dot(pirvf_ss)\n",
    "rpp_pipe = pe_rew / pe_prg\n",
    "\n",
    "print(\"RPP (policy iteration) (this cell):\", rpp_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ad989e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RPP (policy iteration): 0.40428299140506413\n",
      "RPP (value iteration): 0.40428299140459983\n"
     ]
    }
   ],
   "source": [
    "# run the value iteration based pipeline & compare\n",
    "res = util.optimize_and_evaluate(mdp, horizon=100, eps=0.001)\n",
    "\n",
    "print(\"RPP (policy iteration):\", rpp_pipe)\n",
    "print(\"RPP (value iteration):\", res[\"rpp\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
